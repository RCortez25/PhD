{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvbAMmyUalDq4QFhztxomD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/4.%20Attention%20mechanism/3_Multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multihead attention wrapper\n",
        "\n",
        "> In Multi head attention, one divides the attention mechanism into multiple heads, each operating independently.\n",
        ">\n",
        "> One stacks multiple single head attention layers. That is, one creates multiple instances of the self-attention mechanism, each with its own weights and then they are combined.\n",
        ">\n",
        "> Even though this is computationally expensive, it allows the LLM to capture complex patterns."
      ],
      "metadata": {
        "id": "X9_D0sh10oQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "LUE0If9LsVgA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A1eY5uMmyOp3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    # Number of attention heads mut be given. These are the number of single\n",
        "    # self-attention mechanisms\n",
        "    def __init__(self, dimensions_in, dimensions_out, context_length,\n",
        "                 dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # Create a list containing all single self-attention mechanisms. These\n",
        "        # are created using the CausalAttention class\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(dimensions_in, dimensions_out, context_length,\n",
        "                             dropout, qkv_bias) for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    # Method for concatenating all single self-attention heads along the\n",
        "    # columns\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Let's repeat the code we had before to test the class"
      ],
      "metadata": {
        "id": "60oNTLlRsudl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "     [0.22, 0.58, 0.33], # with     (x^4)\n",
        "     [0.77, 0.25, 0.10], # one      (x^5)\n",
        "     [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "batch = torch.stack([inputs, inputs], dim=0)"
      ],
      "metadata": {
        "id": "2eT2s9DmsziV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, dimension_inputs, dimension_outputs, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.dimension_outputs = dimension_outputs\n",
        "        # Initialize the matrices using Linear layers\n",
        "        self.W_q = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_k = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_v = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        # Initialize the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Initialize the buffer, for automatically moving the model to CPU or GPU\n",
        "        self.register_buffer(\"mask\",\n",
        "                             torch.triu(torch.ones(context_length, context_length),\n",
        "                                        diagonal=1))\n",
        "\n",
        "    # Method to calculate the context vector\n",
        "    def forward(self, input_vectors):\n",
        "        # Obtain the relevant dimensions\n",
        "        batch_size, number_of_tokens, dimension_inputs = input_vectors.shape\n",
        "        queries = self.W_q(input_vectors)\n",
        "        keys = self.W_k(input_vectors)\n",
        "        values = self.W_v(input_vectors)\n",
        "\n",
        "        attention_scores = queries @ keys.transpose(1, 2)\n",
        "        attention_scores.masked_fill_(\n",
        "            self.mask.bool()[:number_of_tokens, :number_of_tokens], -torch.inf)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        dimension_keys = keys.shape[-1]\n",
        "        attention_weights = torch.softmax(attention_scores / (dimension_keys ** 0.5), dim=-1)\n",
        "        # Apply dropout\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Calculate and return the context vectors\n",
        "        context_vectors = attention_weights @ values\n",
        "\n",
        "        return context_vectors"
      ],
      "metadata": {
        "id": "DIhS6tcbr2s_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iObp_5ZKsmMc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the wrapper\n",
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1] # Number of tokens\n",
        "dimension_inputs = 3\n",
        "dimension_outputs = 2\n",
        "oMultiHeadAttention = MultiHeadAttentionWrapper(dimension_inputs,\n",
        "                                                dimension_outputs,\n",
        "                                                context_length,\n",
        "                                                dropout=0.0, num_heads=2)\n",
        "context_vectors = oMultiHeadAttention(batch)\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U8J-dTYsYgA",
        "outputId": "feebbc14-5a81-4217-d9fc-a930f2b55160"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4LYOpo9uPxP",
        "outputId": "671c077b-c487-4726-ea87-9d1f3b4dea08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The dimensions here represent:\n",
        "* 2: The number of batches\n",
        "* 6: The number of tokens, or input vectors, in each batch\n",
        "* 4: Ths is because we selected two heads `num_heads=2`. Therefore, since the output dimension of each head is 2, then 2 heads add 2 + 2 = 4."
      ],
      "metadata": {
        "id": "VN52mRBzuRFc"
      }
    }
  ]
}