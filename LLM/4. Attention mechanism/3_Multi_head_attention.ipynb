{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5ANxdjdK0/DB0cr0zNwbh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/4.%20Attention%20mechanism/3_Multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multihead attention wrapper\n",
        "\n",
        "> In Multi head attention, one divides the attention mechanism into multiple heads, each operating independently.\n",
        ">\n",
        "> One stacks multiple single head attention layers. That is, one creates multiple instances of the self-attention mechanism, each with its own weights and then they are combined.\n",
        ">\n",
        "> Even though this is computationally expensive, it allows the LLM to capture complex patterns."
      ],
      "metadata": {
        "id": "X9_D0sh10oQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "LUE0If9LsVgA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A1eY5uMmyOp3"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    # Number of attention heads mut be given. These are the number of single\n",
        "    # self-attention mechanisms\n",
        "    def __init__(self, dimensions_in, dimensions_out, context_length,\n",
        "                 dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # Create a list containing all single self-attention mechanisms. These\n",
        "        # are created using the CausalAttention class\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(dimensions_in, dimensions_out, context_length,\n",
        "                             dropout, qkv_bias) for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    # Method for concatenating all single self-attention heads along the\n",
        "    # columns\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Let's repeat the code we had before to test the class"
      ],
      "metadata": {
        "id": "60oNTLlRsudl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "     [0.22, 0.58, 0.33], # with     (x^4)\n",
        "     [0.77, 0.25, 0.10], # one      (x^5)\n",
        "     [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "batch = torch.stack([inputs, inputs], dim=0)"
      ],
      "metadata": {
        "id": "2eT2s9DmsziV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, dimension_inputs, dimension_outputs, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.dimension_outputs = dimension_outputs\n",
        "        # Initialize the matrices using Linear layers\n",
        "        self.W_q = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_k = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_v = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        # Initialize the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Initialize the buffer, for automatically moving the model to CPU or GPU\n",
        "        self.register_buffer(\"mask\",\n",
        "                             torch.triu(torch.ones(context_length, context_length),\n",
        "                                        diagonal=1))\n",
        "\n",
        "    # Method to calculate the context vector\n",
        "    def forward(self, input_vectors):\n",
        "        # Obtain the relevant dimensions\n",
        "        batch_size, number_of_tokens, dimension_inputs = input_vectors.shape\n",
        "        queries = self.W_q(input_vectors)\n",
        "        keys = self.W_k(input_vectors)\n",
        "        values = self.W_v(input_vectors)\n",
        "\n",
        "        attention_scores = queries @ keys.transpose(1, 2)\n",
        "        attention_scores.masked_fill_(\n",
        "            self.mask.bool()[:number_of_tokens, :number_of_tokens], -torch.inf)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        dimension_keys = keys.shape[-1]\n",
        "        attention_weights = torch.softmax(attention_scores / (dimension_keys ** 0.5), dim=-1)\n",
        "        # Apply dropout\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Calculate and return the context vectors\n",
        "        context_vectors = attention_weights @ values\n",
        "\n",
        "        return context_vectors"
      ],
      "metadata": {
        "id": "DIhS6tcbr2s_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the wrapper\n",
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1] # Number of tokens\n",
        "dimension_inputs = 3\n",
        "dimension_outputs = 2\n",
        "oMultiHeadAttention = MultiHeadAttentionWrapper(dimension_inputs,\n",
        "                                                dimension_outputs,\n",
        "                                                context_length,\n",
        "                                                dropout=0.0, num_heads=2)\n",
        "context_vectors = oMultiHeadAttention(batch)\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U8J-dTYsYgA",
        "outputId": "22dceb92-d140-4a7f-dcd6-26183efb7818"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4LYOpo9uPxP",
        "outputId": "f3d8d73b-2f46-4d95-fc02-b0d9c3211e86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The dimensions here represent:\n",
        "* 2: The number of batches\n",
        "* 6: The number of tokens, or input vectors, in each batch\n",
        "* 4: Ths is because we selected two heads `num_heads=2`. Therefore, since the output dimension of each head is 2, then 2 heads add 2 + 2 = 4."
      ],
      "metadata": {
        "id": "VN52mRBzuRFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head attention with weight splits\n",
        "\n",
        "> Now, the problem with the wrapper is that is not very efficient because of the matrix multiplications involved. We multiply the inputs by two matrices (`num_heads=2`). We can achieve the same result by performing only one matrix multiplication.Let's combine the two classes CausalAttention and MultiHeadAttentionWrapper together and add some other modifications."
      ],
      "metadata": {
        "id": "2VFMEEO82u1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dimension_inputs, dimension_outputs, context_length,\n",
        "                 dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (dimension_outputs % num_heads == 0), \"Dimension outputs must be \\\n",
        "                                                    divisible by number of heads\" # 1\n",
        "        self.dimension_outputs = dimension_outputs\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dimension = dimension_outputs // num_heads # 2\n",
        "\n",
        "        #Initialize weight matrices\n",
        "        self.W_query = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        # Initialize a linear layer to combine head outputs\n",
        "        self.output_projection = nn.Linear(dimension_outputs, dimension_outputs) # 3\n",
        "        # Initialize the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\",\n",
        "                             torch.triu(torch.ones(context_length, context_length),\n",
        "                                        diagonal=1))\n",
        "\n",
        "    # Method for calculating the context vectors\n",
        "    def forward(self, input_vectors):\n",
        "        batch_size, number_of_tokens, dimension_inputs = input_vectors.shape\n",
        "\n",
        "        # Calculate keys, queries, and values\n",
        "        queries = self.W_query(input_vectors)\n",
        "        keys = self.W_key(input_vectors)\n",
        "        values = self.W_value(input_vectors)\n",
        "\n",
        "        # Split the obtained matrices by adding a \"number of heads\" dimension\n",
        "        # that is, the output dimension will be splitted as\n",
        "        # (batch_size, number_of_tokens, dimension_outputs) will be\n",
        "        # (batch_size, number_of_tokens, num_heads, head_dimension)\n",
        "        keys = keys.view(batch_size, number_of_tokens, self.num_heads, self.head_dimension)\n",
        "        values = values.view(batch_size, number_of_tokens, self.num_heads, self.head_dimension)\n",
        "        queries = queries.view(batch_size, number_of_tokens, self.num_heads, self.head_dimension)\n",
        "\n",
        "        # Transpose for performing calculations\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "        # TO-DO\n",
        "        attention_scores.masked_fill_(\n",
        "            self.mask.bool()[:number_of_tokens, :number_of_tokens], -torch.inf)\n",
        "\n",
        "        # Calculate attention"
      ],
      "metadata": {
        "id": "cSaNH15H2uV3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Comments:\n",
        ">\n",
        "> * 1: Use the `assert` to ensure that dimension is correct\n",
        "* 2: Calculate the dimension of each head\n",
        "* 3:  "
      ],
      "metadata": {
        "id": "ntLLnb6Ciq22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple example of multihead attention\n",
        "\n",
        "## Step 1: Inputs\n",
        "\n",
        "Say we have 1 batch of 3 tokens, each in a 6-dimensional space. Say the input batch is \"The cat sleeps\". That is\n",
        "\n",
        "`batch_size=1` \\\n",
        "`number_of_tokens=3` \\\n",
        "`dimension_inputs=6`"
      ],
      "metadata": {
        "id": "xC_hjXmrgV0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[\n",
        "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], # The\n",
        "    [6.0, 5.0, 4.0, 3.0, 2.0, 1.0], # cat\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  # sleeps\n",
        "]])"
      ],
      "metadata": {
        "id": "LGCsUfZ65F1E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Decide the output dimension and the number of heads\n",
        "\n",
        "`dimension_outputs=6` \\\n",
        "`num_heads=2`\n",
        "\n",
        "Recall the the output dimension is the dimension of the context vector for each input token. Typically, in GPT models `dimension_input=dimension_outputs`.\n",
        "\n",
        "For GPT models, the number of heads is 96, but in this example we'll keep it simple.\n",
        "\n",
        "This decision gives\n",
        "\n",
        "`head_dimension = dimension_outputs/num_heads = 6/2 = 3`\n",
        "\n",
        "## Spet 3: Initialize trainable weight matrices\n",
        "\n",
        "Initialize `W_query, W_key_, W_value`. Their dimension must be\n",
        "\n",
        "$$dimension\\_inputs\\times dimension\\_outputs$$\n",
        "\n",
        "so, in this case one has\n",
        "\n",
        "$$6\\times6$$\n",
        "\n",
        "this is done so in order to make matrix multiplication plausible."
      ],
      "metadata": {
        "id": "dpcNfOQWhYSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_query = nn.Linear(6, 6)\n",
        "W_key = nn.Linear(6, 6)\n",
        "W_value = nn.Linear(6, 6)\n",
        "\n",
        "print(W_query)\n",
        "print(W_key)\n",
        "print(W_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGbjnjxHg_wj",
        "outputId": "d21266d9-f0cc-497e-b1e9-b12cba7df49d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=6, out_features=6, bias=True)\n",
            "Linear(in_features=6, out_features=6, bias=True)\n",
            "Linear(in_features=6, out_features=6, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the actual values of the matrices. Note that they contain bias which in this case is not important."
      ],
      "metadata": {
        "id": "y-Mb6YWwkrpK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f225f9",
        "outputId": "6f36a61e-a3bf-4aab-d2f7-8ef58218bc35"
      },
      "source": [
        "print(\"W_query weights:\\n\", W_query.weight)\n",
        "print(\"W_query bias:\\n\", W_query.bias)\n",
        "\n",
        "print(\"\\nW_key weights:\\n\", W_key.weight)\n",
        "print(\"W_key bias:\\n\", W_key.bias)\n",
        "\n",
        "print(\"\\nW_value weights:\\n\", W_value.weight)\n",
        "print(\"W_value bias:\\n\", W_value.bias)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_query weights:\n",
            " Parameter containing:\n",
            "tensor([[-0.2389,  0.3859, -0.1704,  0.0871, -0.1995, -0.1969],\n",
            "        [ 0.1828, -0.1140, -0.2589, -0.1669,  0.2977,  0.2458],\n",
            "        [ 0.2485, -0.3484,  0.1923,  0.1019, -0.2745,  0.0129],\n",
            "        [ 0.0816, -0.2205, -0.1723,  0.3330, -0.0330, -0.0043],\n",
            "        [-0.2584, -0.2441,  0.3759,  0.1519, -0.0646,  0.2487],\n",
            "        [-0.1942, -0.3561, -0.4053,  0.1659, -0.1592,  0.2507]],\n",
            "       requires_grad=True)\n",
            "W_query bias:\n",
            " Parameter containing:\n",
            "tensor([ 0.3488,  0.1345,  0.3508, -0.0940,  0.3558, -0.1947],\n",
            "       requires_grad=True)\n",
            "\n",
            "W_key weights:\n",
            " Parameter containing:\n",
            "tensor([[-0.0536,  0.2713, -0.2115,  0.3115,  0.1001, -0.0080],\n",
            "        [ 0.3494,  0.3062, -0.1679,  0.0396,  0.0476,  0.3344],\n",
            "        [ 0.2294,  0.3306,  0.2489, -0.3552,  0.2712, -0.1084],\n",
            "        [ 0.3275,  0.2569, -0.2386, -0.0430,  0.0609,  0.1167],\n",
            "        [-0.3781,  0.0183,  0.2127,  0.2305,  0.2008,  0.0646],\n",
            "        [-0.3916,  0.2686, -0.3215, -0.2399,  0.0048,  0.1243]],\n",
            "       requires_grad=True)\n",
            "W_key bias:\n",
            " Parameter containing:\n",
            "tensor([ 0.2372, -0.0573, -0.2101, -0.0351,  0.1338, -0.2297],\n",
            "       requires_grad=True)\n",
            "\n",
            "W_value weights:\n",
            " Parameter containing:\n",
            "tensor([[-0.3546,  0.1949, -0.2702, -0.2298,  0.3387, -0.2690],\n",
            "        [-0.3313,  0.3103, -0.1948,  0.0265,  0.4067, -0.1629],\n",
            "        [ 0.3803,  0.3244,  0.3153,  0.1211, -0.1840,  0.2570],\n",
            "        [-0.2796, -0.2378, -0.1968,  0.1766,  0.0562,  0.2597],\n",
            "        [ 0.2683,  0.0238,  0.2380, -0.2950, -0.3902, -0.3326],\n",
            "        [ 0.2253,  0.3754, -0.1129,  0.2259, -0.2917, -0.0077]],\n",
            "       requires_grad=True)\n",
            "W_value bias:\n",
            " Parameter containing:\n",
            "tensor([-0.0025, -0.1182, -0.1979,  0.1915, -0.0356, -0.0809],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Calculate Keys, Queries, and Values matrices\n",
        "\n",
        "$$inputs * W_{keys}$$\n",
        "$$inputs * W_{queries}$$\n",
        "$$inputs * W_{values}$$\n",
        "\n",
        "The resulting dimensions will be\n",
        "\n",
        "$$1\\times3\\times6$$"
      ],
      "metadata": {
        "id": "3yTuUo93lKkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Keys = W_key(x)\n",
        "print(\"Keys:\\n\", Keys)\n",
        "\n",
        "Queries = W_query(x)\n",
        "print(\"\\nQueries:\\n\", Queries)\n",
        "\n",
        "Values = W_value(x)\n",
        "print(\"\\nValues:\\n\", Values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx5p9WZilLi_",
        "outputId": "f590e78a-ebec-4c2b-dfa0-f1248a5c1743"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys:\n",
            " tensor([[[ 1.7901,  2.8035,  0.7118,  0.9231,  2.7435, -1.2384],\n",
            "         [ 1.5530,  3.4468,  3.1830,  2.3697, -0.0352, -3.1080],\n",
            "         [ 0.6470,  0.8520,  0.4063,  0.4453,  0.4825, -0.7850]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "Queries:\n",
            " tensor([[[-1.4601,  1.6082, -0.4082,  0.1709,  2.5136, -0.9453],\n",
            "         [-0.1715, -0.0334,  0.6383, -0.4673, -0.3364, -4.3318],\n",
            "         [ 0.0160,  0.3210,  0.2834, -0.1095,  0.5651, -0.8929]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "Values:\n",
            " tensor([[[-1.6174,  0.7487,  2.8835,  1.3917, -4.1323, -0.0449],\n",
            "         [-2.5170, -0.6037,  5.2189, -2.5608,  0.6473,  2.7826],\n",
            "         [-0.5924, -0.0637,  1.0161, -0.0302, -0.5233,  0.3333]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Unroll the last dimension of the Keys, Queries, Values matrices to include the number of heads and the dimension of heads.\n",
        "\n",
        "\n",
        "(batch_size, number_of_tokens, dimension_outputs) ->\n",
        "(batch_size, number_of_tokens, num_heads, head_dimension)\n",
        "\n",
        "that is\n",
        "\n",
        "(1, 3, 6) ->\n",
        "(1, 3, 2, 3)\n"
      ],
      "metadata": {
        "id": "t069_Xwyswwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_queries = Queries.view(1, 3, 2, 3)\n",
        "reshaped_keys = Keys.view(1, 3, 2, 3)\n",
        "reshaped_values = Values.view(1, 3, 2, 3)"
      ],
      "metadata": {
        "id": "dR1yOoVlr8-q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Reshaped queries:\\n\", reshaped_queries)\n",
        "print(\"\\nReshaped keys:\\n\", reshaped_keys)\n",
        "print(\"\\nReshaped values:\\n\", reshaped_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgHdEYamuEFv",
        "outputId": "af266d06-8b6f-4839-cd65-3e318e1722c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reshaped queries:\n",
            " tensor([[[[-1.4601,  1.6082, -0.4082],\n",
            "          [ 0.1709,  2.5136, -0.9453]],\n",
            "\n",
            "         [[-0.1715, -0.0334,  0.6383],\n",
            "          [-0.4673, -0.3364, -4.3318]],\n",
            "\n",
            "         [[ 0.0160,  0.3210,  0.2834],\n",
            "          [-0.1095,  0.5651, -0.8929]]]], grad_fn=<ViewBackward0>)\n",
            "\n",
            "Reshaped keys:\n",
            " tensor([[[[ 1.7901,  2.8035,  0.7118],\n",
            "          [ 0.9231,  2.7435, -1.2384]],\n",
            "\n",
            "         [[ 1.5530,  3.4468,  3.1830],\n",
            "          [ 2.3697, -0.0352, -3.1080]],\n",
            "\n",
            "         [[ 0.6470,  0.8520,  0.4063],\n",
            "          [ 0.4453,  0.4825, -0.7850]]]], grad_fn=<ViewBackward0>)\n",
            "\n",
            "Reshaped values:\n",
            " tensor([[[[-1.6174,  0.7487,  2.8835],\n",
            "          [ 1.3917, -4.1323, -0.0449]],\n",
            "\n",
            "         [[-2.5170, -0.6037,  5.2189],\n",
            "          [-2.5608,  0.6473,  2.7826]],\n",
            "\n",
            "         [[-0.5924, -0.0637,  1.0161],\n",
            "          [-0.0302, -0.5233,  0.3333]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first groud of two elements of each matrix corresponds to the first token, and each element corresponds to one head. For instance, for the reshaped queries matrix:\n",
        "\n",
        "[-1.4601,  1.6082, -0.4082], \\\n",
        "[ 0.1709,  2.5136, -0.9453]\n",
        "\n",
        "corresponds to the first token \"The\", and each row corresponds to one head each. Then\n",
        "\n",
        "[-0.1715, -0.0334,  0.6383], \\\n",
        "[-0.4673, -0.3364, -4.3318]\n",
        "\n",
        "corresponds to the second token \"cat\", and each row corresponds to one head each. And so on.\n",
        "\n",
        "Lastly, each token is now in 3 dimensions, hence the 3 elements of each row, because the head dimension equals 3.\n",
        "\n",
        "Recall then that each head pays attention to each token separatedly."
      ],
      "metadata": {
        "id": "BePDz3WcuhWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Group by heads\n",
        "\n",
        "For performing calculations we need to reshape the matrices to be of the following format\n",
        "\n",
        "(batch_size, number_of_tokens, num_heads, head_dimension) ->\n",
        "(batch_size, num_heads, number_of_tokens, head_dimension)\n",
        "\n",
        "that is\n",
        "\n",
        "(1, 3, 2, 3) ->\n",
        "(1, 2, 3, 3)\n",
        "\n",
        "Now, given that we have indexes 0,1,2,3 and we want to interchange indexes 1 and 2, we transpose the matrix using these indexes."
      ],
      "metadata": {
        "id": "OGwUam8TwMsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transposed_queries = reshaped_queries.transpose(1, 2) # This means interchanging indexes 1 and 2\n",
        "transposed_keys = reshaped_keys.transpose(1, 2)\n",
        "transposed_values = reshaped_values.transpose(1, 2)"
      ],
      "metadata": {
        "id": "GO3NeS_euFU6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transposed queries:\\n\", transposed_queries)\n",
        "print(\"\\nTransposed keys:\\n\", transposed_keys)\n",
        "print(\"\\nTransposed values:\\n\", transposed_values)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dgJTT_UxFmq",
        "outputId": "15115bb0-16d1-4a41-c98b-f53fa944a2d4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transposed queries:\n",
            " tensor([[[[-1.4601,  1.6082, -0.4082],\n",
            "          [-0.1715, -0.0334,  0.6383],\n",
            "          [ 0.0160,  0.3210,  0.2834]],\n",
            "\n",
            "         [[ 0.1709,  2.5136, -0.9453],\n",
            "          [-0.4673, -0.3364, -4.3318],\n",
            "          [-0.1095,  0.5651, -0.8929]]]], grad_fn=<TransposeBackward0>)\n",
            "\n",
            "Transposed keys:\n",
            " tensor([[[[ 1.7901,  2.8035,  0.7118],\n",
            "          [ 1.5530,  3.4468,  3.1830],\n",
            "          [ 0.6470,  0.8520,  0.4063]],\n",
            "\n",
            "         [[ 0.9231,  2.7435, -1.2384],\n",
            "          [ 2.3697, -0.0352, -3.1080],\n",
            "          [ 0.4453,  0.4825, -0.7850]]]], grad_fn=<TransposeBackward0>)\n",
            "\n",
            "Transposed values:\n",
            " tensor([[[[-1.6174,  0.7487,  2.8835],\n",
            "          [-2.5170, -0.6037,  5.2189],\n",
            "          [-0.5924, -0.0637,  1.0161]],\n",
            "\n",
            "         [[ 1.3917, -4.1323, -0.0449],\n",
            "          [-2.5608,  0.6473,  2.7826],\n",
            "          [-0.0302, -0.5233,  0.3333]]]], grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in the result, the first element of each matrix corresponds to the 3 tokens for the first head, and the second element corresponds to the 3 tokens for the second head.\n",
        "\n",
        "That is, for the queries matrix\n",
        "\n",
        "[[-1.4601,  1.6082, -0.4082], \\\n",
        "[-0.1715, -0.0334,  0.6383], \\\n",
        "[ 0.0160,  0.3210,  0.2834]]\n",
        "\n",
        "corresponds to the 3 tokens for the first head, and\n",
        "\n",
        "[[ 0.1709,  2.5136, -0.9453], \\\n",
        "[-0.4673, -0.3364, -4.3318], \\\n",
        "[-0.1095,  0.5651, -0.8929]]\n",
        "\n",
        "corresponds to the 3 tokens for the second head."
      ],
      "metadata": {
        "id": "7GUA3qQgxwyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Find attention scores (self-attention)\n",
        "\n",
        "$$queries * keys.transpose(2,3)$$\n",
        "\n",
        "We need to transpose"
      ],
      "metadata": {
        "id": "3zblYxGsymvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores = transposed_queries @ transposed_keys.transpose(2, 3)\n",
        "print(attention_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBrCjgl5xHPC",
        "outputId": "a064861e-87a1-467c-e812-ecbe572238cb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 1.6042,  1.9762,  0.2595],\n",
            "          [ 0.0538,  1.6504,  0.1200],\n",
            "          [ 1.1305,  2.0336,  0.3990]],\n",
            "\n",
            "         [[ 8.2246,  3.2545,  2.0309],\n",
            "          [ 4.0104, 12.3678,  3.0300],\n",
            "          [ 2.5552,  2.4958,  0.9248]]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, each big block corresponds to each head. These block corresponds to attention scores. Recall that each row is a word and each column is also a word.\n",
        "\n",
        "So the first element 1.6042 corresponds to the attention score between \"The\" and \"The\". The element 1.9762 is the attention score between \"The\" and \"cat\". The element 0.1200 corresponds to the attention score beteween \"cat\" and \"sleeps\". All these correspond to the first head and the same applies for the second big block which corresponds to the second head."
      ],
      "metadata": {
        "id": "ZpHdAocX1SGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Calculate attention weights\n",
        "\n",
        "Recall that in causal attention one masks future tokens. Firts, we mask future tokens, then we divide by the \"scaled dot-product self-attention\", then we apply softmax."
      ],
      "metadata": {
        "id": "cTA-DpU62-yg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41723ff2",
        "outputId": "81aa760d-88d7-417d-ff7d-e17f9bc29176"
      },
      "source": [
        "mask = torch.triu(torch.ones(attention_scores.size(-2), attention_scores.size(-1)), diagonal=1).bool()\n",
        "attention_scores_masked = attention_scores.masked_fill(mask, -torch.inf)\n",
        "print(\"Masked attention scores:\\n\", attention_scores_masked)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked attention scores:\n",
            " tensor([[[[ 1.6042,    -inf,    -inf],\n",
            "          [ 0.0538,  1.6504,    -inf],\n",
            "          [ 1.1305,  2.0336,  0.3990]],\n",
            "\n",
            "         [[ 8.2246,    -inf,    -inf],\n",
            "          [ 4.0104, 12.3678,    -inf],\n",
            "          [ 2.5552,  2.4958,  0.9248]]]], grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143065a9",
        "outputId": "4c6e8a22-4744-4296-9943-7eed64e978b6"
      },
      "source": [
        "head_dimension = transposed_keys.size(-1) # Get the head dimension\n",
        "attention_scores_scaled = attention_scores_masked / (head_dimension ** 0.5)\n",
        "print(\"Scaled attention scores:\\n\", attention_scores_scaled)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled attention scores:\n",
            " tensor([[[[0.9262,   -inf,   -inf],\n",
            "          [0.0311, 0.9529,   -inf],\n",
            "          [0.6527, 1.1741, 0.2304]],\n",
            "\n",
            "         [[4.7485,   -inf,   -inf],\n",
            "          [2.3154, 7.1406,   -inf],\n",
            "          [1.4752, 1.4410, 0.5339]]]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c0fc614",
        "outputId": "3dded549-8517-4ce5-dd1b-281129376d91"
      },
      "source": [
        "attention_weights = torch.softmax(attention_scores_scaled, dim=-1)\n",
        "print(\"Attention weights:\\n\", attention_weights)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:\n",
            " tensor([[[[1.0000, 0.0000, 0.0000],\n",
            "          [0.2846, 0.7154, 0.0000],\n",
            "          [0.2994, 0.5043, 0.1963]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.0080, 0.9920, 0.0000],\n",
            "          [0.4244, 0.4101, 0.1656]]]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, one could implement dropout, but for the sake of simplicity we will leave it as is."
      ],
      "metadata": {
        "id": "o4M2gPp17fXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9:"
      ],
      "metadata": {
        "id": "QrR4Dm-D7R4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QlANSXEY7sdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}