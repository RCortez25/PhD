{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtXnZoEw0smHyz1E3wB/jw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/4.%20Attention%20mechanism/0_Simplified_attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The importance that the model gives to a certain word is given by the attention weights.\n",
        "\n",
        "\n",
        "Self-attention is the key component of modern LLMs. The \"self\" refers to the ability to compute attention weights by relationg different positions of the same input. Traditional attention mechanisms focus on relationships between elements in different sequences (like input and output).\n",
        "\n",
        "The goal: Calculate a context vector $z^{(i)}$ for each input vector $x^{(i)}$. This context vector is an enriched version of each input vector which contains information about relationships of tokens.\n",
        "___"
      ],
      "metadata": {
        "id": "uNwfg8xC7h-d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAM2ijzloknu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create the inpur tensors\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "     [0.22, 0.58, 0.33], # with     (x^4)\n",
        "     [0.77, 0.25, 0.10], # one      (x^5)\n",
        "     [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    }
  ]
}