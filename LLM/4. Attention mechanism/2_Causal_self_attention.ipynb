{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe6sybUz8dNMgJI8NLi6Ow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/4.%20Attention%20mechanism/2_Causal_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal self-attention\n",
        "\n",
        "> Causal self-attention is a special form of self-attention. It restricts the model to only consider the previous and current tokens for the analysis. Self-attention calculates, for instance, the attention weights for a given query but in relation to all other tokens, before and after the query. That is, we mask out future tokens and only consider the query and the tokens before it. What we need to modify are the attention weights.\n",
        "\n",
        "> Let's reuse the code we had for the self-attention mechanism.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A4l4HovZnHbf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0iNQ7TWPlg8W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create the input tensors\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "     [0.22, 0.58, 0.33], # with     (x^4)\n",
        "     [0.77, 0.25, 0.10], # one      (x^5)\n",
        "     [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "class SelfAttentionV2(nn.Module):\n",
        "    # Initialize the object with the number of input and output dimensions\n",
        "    # Initialize the bias to False\n",
        "    def __init__(self, dimension_inputs, dimension_outputs, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # Initialize the matrices using Linear layers\n",
        "        self.W_q = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_k = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_v = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "\n",
        "    # Method to calculate the context vector\n",
        "    def forward(self, input_vectors):\n",
        "        # Calculate the query, key, and value vectors by calling the Linear layers\n",
        "        # Note that in this case we don't perform a direct matrix multiplication\n",
        "        # but rather just call the Linear layer as a function passing in the\n",
        "        # input vectors. The Linear layer performs the matrix multiplication\n",
        "        queries = self.W_q(input_vectors)\n",
        "        keys = self.W_k(input_vectors)\n",
        "        values = self.W_v(input_vectors)\n",
        "\n",
        "        # Calculate attention scores, that is, omega\n",
        "        attention_scores = queries @ keys.T\n",
        "\n",
        "        # Calculate attention weights\n",
        "        dimension_keys = keys.shape[-1]\n",
        "        attention_weights = torch.softmax(attention_scores / (dimension_keys ** 0.5), dim=-1)\n",
        "\n",
        "        # Calculate and return the context vectors\n",
        "        context_vectors = attention_weights @ values\n",
        "\n",
        "        return context_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Let's use this class to obtain the attention weights."
      ],
      "metadata": {
        "id": "tCLQEvHlrAkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the class\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Obtain queries and keys\n",
        "oSelfAttentionV2 = SelfAttentionV2(3, 2)\n",
        "queries = oSelfAttentionV2.W_q(inputs)\n",
        "keys = oSelfAttentionV2.W_k(inputs)\n",
        "\n",
        "# Calculate attention scores\n",
        "attention_scores = queries @ keys.T\n",
        "\n",
        "# Calculate attention weights\n",
        "dimension_keys = keys.shape[-1]\n",
        "attention_weights = torch.softmax(attention_scores / (dimension_keys ** 0.5), dim=-1)\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE0pHpvFp20_",
        "outputId": "865d8c98-7d3e-462e-9b9f-15317d489697"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
              "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
              "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
              "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
              "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
              "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now, create a mask to hide future tokens"
      ],
      "metadata": {
        "id": "aCvhG_K7r2o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the context length\n",
        "context_length = inputs.shape[0]\n",
        "\n",
        "# Create the mask using tril (triangular lower) function of PyTorch\n",
        "# torch.ones creates a tensor of 1s, and the tril function zeroes out\n",
        "# the upper elements\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "mask_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMtqknuQrnxZ",
        "outputId": "2b9b05b5-8d86-4d2a-816d-7fa699d6b3bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the simple mask by simply multiplying the two tensors\n",
        "attention_weights_simple_masked = attention_weights * mask_simple\n",
        "attention_weights_simple_masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RCNIBIvsbb1",
        "outputId": "fe1ac091-025f-4244-a398-e5eb2ece2d04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
              "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> However, the results are not normalized as required, so we will normalize them"
      ],
      "metadata": {
        "id": "R-60q6dquAHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the masked attention weights\n",
        "attention_weights_normalized = attention_weights_simple_masked / attention_weights_simple_masked.sum(dim=-1, keepdim=True)\n",
        "attention_weights_normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdzajXD7tZXD",
        "outputId": "055b57d9-1c0f-4ca9-c938-251240d20ed8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
              "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
              "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now, it happens that we have a data leakage problem since before all this masking, the softmax function was applied to the attention weights, and this caused future tokens to influence the tokens not masked out. In this case, one solves the problem by using an upper triangular infinity matrix. This way, the tokens of interest don't change their value, only those in the upper diagonal. Then, after applying the softmax those negative infinities are treated as zero not affecting the tokens of interest."
      ],
      "metadata": {
        "id": "c-JmzNpUuzYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the upper triangular (triu) mask with 1s in the upper part of it\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNd3iV_XuQtO",
        "outputId": "b2c87385-b400-4b05-dc3e-313fe318a545"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask out the attention scores using negative infinity\n",
        "# mask.bool() returns True where there's no 0, then masked_fill replaces those\n",
        "# values with -infinity\n",
        "attention_scores_masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "attention_scores_masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzimCe-uv4O8",
        "outputId": "8fe75932-0f24-4292-b2cf-8b9324710fbc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
              "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
              "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
              "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
              "       grad_fn=<MaskedFillBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the original attention scores to compare that the numbers of interest\n",
        "# were not affected\n",
        "attention_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT_MWgYbwJ37",
        "outputId": "34628035-0314-499b-8e41-84ee08368343"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3111, 0.3479, 0.3471, 0.1714, 0.2350, 0.1928],\n",
              "        [0.1655, 0.2602, 0.2576, 0.1445, 0.1384, 0.1790],\n",
              "        [0.1667, 0.2602, 0.2577, 0.1443, 0.1391, 0.1784],\n",
              "        [0.0510, 0.1080, 0.1064, 0.0643, 0.0476, 0.0835],\n",
              "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121, 0.1174],\n",
              "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applyt the softmaxk to the masked attention weights\n",
        "attention_weights_masked = torch.softmax(attention_scores_masked/(dimension_keys ** 0.5), dim=-1)\n",
        "attention_weights_masked\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjt3r_9Pwkgs",
        "outputId": "e0b2d997-fe1d-43e2-edb5-4d65c163aa63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
              "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
              "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking that all rows sum up to 1\n",
        "attention_weights_masked.sum(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffx5IPt_w11r",
        "outputId": "f6b2ef05-dbe7-4785-dc59-af8e1ba9fbbe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000]], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "\n",
        "> We will use this technique from Deep Learning to mask additional attention weights in order to prevent overfitting and improve generalization. In transformers architectures, the dropout can be applied after calculating attention weights and after applying the attention weights to value vectors. In this case, we will apply it after after calculating attention scores."
      ],
      "metadata": {
        "id": "fbZGO1ls0Vyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a dropout layer with a dropout rate of 50%, that is, to mak out\n",
        "# 50% of the attention weights\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "# Create an example for demonstration purposes\n",
        "example = torch.ones(6, 6)\n",
        "\n",
        "# Apply dropout\n",
        "dropout(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-sTw_3gykRG",
        "outputId": "853a9438-b27f-47da-ea3e-9bcd487392fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 2., 2., 2., 2.],\n",
              "        [2., 0., 2., 0., 2., 0.],\n",
              "        [0., 0., 2., 2., 2., 0.],\n",
              "        [2., 2., 0., 2., 0., 2.],\n",
              "        [2., 0., 2., 2., 2., 2.],\n",
              "        [2., 2., 2., 0., 2., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note that in order to account for the dropout, the values remaining that are not zeroed-out are 1s, but since 50% were zeroed-out, those 1s are divided by 0.5, producing the 2s we see in the resulting tensor. Now, let us apply it to the attention weights."
      ],
      "metadata": {
        "id": "ocAmey7VAfQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This seed doesn't zero-out the first element of the first row, which is 1\n",
        "# And this shows that the resulting value is scaled and results in a 2 after\n",
        "# applying the dropout\n",
        "torch.manual_seed(123)\n",
        "print(attention_weights_masked)\n",
        "print(dropout(attention_weights_masked))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FkpOlVOAKLw",
        "outputId": "46aca341-b715-45a0-9fd1-598e4bb80c67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
            "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0335, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4889, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3418, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal attention class\n",
        "\n",
        "> To create this class, first we need to make sure the code can handle batches of inputs. In this case we will simply duplicate the inputs we already have. We will have then 2 inputs with 6 tokens each, and each token in a 3D space."
      ],
      "metadata": {
        "id": "jSMsQ7pzCZFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a batch by simply duplicating the inputs\n",
        "batch = torch.stack([inputs, inputs], dim=0)\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj59g2mKBXTx",
        "outputId": "52bac6b5-05d7-42da-85d9-334dd47b677a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]],\n",
              "\n",
              "        [[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ukTigNPDAkk",
        "outputId": "b6b4db00-b325-4172-837f-4bbfa791e254"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now, the CausalAttention class is similar to the SelfAttentionV2 class."
      ],
      "metadata": {
        "id": "IXCQT1FADPHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    # Initialize the object with the number of input and output dimensions\n",
        "    # Initialize the bias to False\n",
        "    # Initialize a dropout rate\n",
        "    # Initialize the context length\n",
        "    def __init__(self, dimension_inputs, dimension_outputs, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.dimension_outputs = dimension_outputs\n",
        "        # Initialize the matrices using Linear layers\n",
        "        self.W_q = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_k = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        self.W_v = nn.Linear(dimension_inputs, dimension_outputs, bias=qkv_bias)\n",
        "        # Initialize the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Initialize the buffer, for automatically moving the model to CPU or GPU\n",
        "        self.register_buffer(\"mask\",\n",
        "                             torch.triu(torch.ones(context_length, context_length),\n",
        "                                        diagonal=1))\n",
        "\n",
        "    # Method to calculate the context vector\n",
        "    def forward(self, input_vectors):\n",
        "        # Obtain the relevant dimensions\n",
        "        batch_size, number_of_tokens, dimension_inputs = input_vectors.shape\n",
        "        # Calculate the query, key, and value vectors by calling the Linear layers\n",
        "        # Note that in this case we don't perform a direct matrix multiplication\n",
        "        # but rather just call the Linear layer as a function passing in the\n",
        "        # input vectors. The Linear layer performs the matrix multiplication\n",
        "        queries = self.W_q(input_vectors)\n",
        "        keys = self.W_k(input_vectors)\n",
        "        values = self.W_v(input_vectors)\n",
        "\n",
        "        # Calculate attention scores. Note that this way of transposing the keys\n",
        "        # is because we need to rearrange the batches to perform matrix\n",
        "        # multiplication, otherwise dimensions don't match\n",
        "        attention_scores = queries @ keys.transpose(1, 2)\n",
        "        # Now mask out the future tokens\n",
        "        # masked_fill_ with the underscore makes the function to modify the\n",
        "        # tensor in-place\n",
        "        attention_scores.masked_fill_(\n",
        "            self.mask.bool()[:number_of_tokens, :number_of_tokens], -torch.inf)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        dimension_keys = keys.shape[-1]\n",
        "        attention_weights = torch.softmax(attention_scores / (dimension_keys ** 0.5), dim=-1)\n",
        "        # Apply dropout\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Calculate and return the context vectors\n",
        "        context_vectors = attention_weights @ values\n",
        "\n",
        "        return context_vectors"
      ],
      "metadata": {
        "id": "u1l4T7HrDKnq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the class\n",
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "oCausalAttention = CausalAttention(dimension_inputs=3, dimension_outputs=2,\n",
        "                                   context_length=context_length, dropout=0.0)\n",
        "oCausalAttention(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJwtzbPl5SIy",
        "outputId": "8830fe6d-d4b9-4033-f25f-9f20e0556166"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4519,  0.2216],\n",
              "         [-0.5874,  0.0058],\n",
              "         [-0.6300, -0.0632],\n",
              "         [-0.5675, -0.0843],\n",
              "         [-0.5526, -0.0981],\n",
              "         [-0.5299, -0.1081]],\n",
              "\n",
              "        [[-0.4519,  0.2216],\n",
              "         [-0.5874,  0.0058],\n",
              "         [-0.6300, -0.0632],\n",
              "         [-0.5675, -0.0843],\n",
              "         [-0.5526, -0.0981],\n",
              "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}