{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt06onEVuWLxpR+fvBhVNH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/4.%20Attention%20mechanism/1_Self_attention_with_trainable_weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This kind of self attention mechanism is also called **scaled dot-product attention**.\n",
        "\n",
        "Now, we introduce trainable weight matrices that are updated during training. The are used so that the model learns to produce good context vectors. These matrices are:\n",
        "\n",
        "* $W_q$ for queries\n",
        "* $W_k$ for keys\n",
        "* $W_v$ for values\n",
        "\n",
        "These project the input vectors $x^{(i)}$ into yet another new query, key, and value vectors for each input vector."
      ],
      "metadata": {
        "id": "Cz4QozwqY6R4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SdSdhTsUXHpN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create the input tensors\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "     [0.22, 0.58, 0.33], # with     (x^4)\n",
        "     [0.77, 0.25, 0.10], # one      (x^5)\n",
        "     [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the second input vector for demonstration purposes\n",
        "x_2 = inputs[1]\n",
        "\n",
        "# Select the dimension of the inputs. 3D in this case\n",
        "dimension_inputs = inputs.shape[1]\n",
        "\n",
        "# Select the output dimension for the projected vectors. 2D in this case\n",
        "dimension_outputs = 2"
      ],
      "metadata": {
        "id": "Mn6U85m4ch3g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the weight matrices, initialized in a random manner\n",
        "# requires_grad=False because we're not training yet\n",
        "torch.manual_seed(1)\n",
        "W_q = torch.nn.Parameter(torch.randn(dimension_inputs, dimension_outputs), requires_grad=False)\n",
        "W_k = torch.nn.Parameter(torch.randn(dimension_inputs, dimension_outputs), requires_grad=False)\n",
        "W_v = torch.nn.Parameter(torch.randn(dimension_inputs, dimension_outputs), requires_grad=False)"
      ],
      "metadata": {
        "id": "J0Ssmw81c25k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvxEUQxCdXDp",
        "outputId": "9991070d-fd1e-4553-e932-ace0ff858227"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.6614,  0.2669],\n",
            "        [ 0.0617,  0.6213],\n",
            "        [-0.4519, -0.1661]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjG8UQVd8fk",
        "outputId": "1ab1e2d4-911f-420c-9325-749595c6f857"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-1.5228,  0.3817],\n",
            "        [-1.0276, -0.5631],\n",
            "        [-0.8923, -0.0583]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx2ZDt9rd-GC",
        "outputId": "50f95c6c-3a24-4fd3-c403-8cc76f2f3cfa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1955, -0.9656],\n",
            "        [ 0.4224,  0.2673],\n",
            "        [-0.4212, -0.5107]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in this case, let's obtain the query, key, and value vectors for the input vector $x^{(2)}$ corresponding to \"journey\"."
      ],
      "metadata": {
        "id": "R1uWWICUeuBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = x_2 @ W_q\n",
        "key_2 = x_2 @ W_k\n",
        "value_2 = x_2 @ W_v\n",
        "\n",
        "print(query_2)\n",
        "print(key_2)\n",
        "print(value_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_x_e7XGd_bt",
        "outputId": "f09a0c61-02ad-4d67-ca8d-ac70f13406d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1191, 0.5777])\n",
            "tensor([-2.3205, -0.3184])\n",
            "tensor([-0.0180, -0.6356])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, obtain the key, query, and value vectors for all input vectors at once."
      ],
      "metadata": {
        "id": "fZvpXYkpfS1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_k\n",
        "queries = inputs @ W_q\n",
        "values = inputs @ W_v\n",
        "\n",
        "print(\"Keys:\")\n",
        "print(keys)\n",
        "print(\"Queries:\")\n",
        "print(queries)\n",
        "print(\"Values:\")\n",
        "print(values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8lp5Vone_5P",
        "outputId": "c6beff1c-bca5-4212-aec6-e84c14abee5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys:\n",
            "tensor([[-1.6031,  0.0278],\n",
            "        [-2.3205, -0.3184],\n",
            "        [-2.3125, -0.2983],\n",
            "        [-1.2255, -0.2618],\n",
            "        [-1.5187,  0.1473],\n",
            "        [-1.3890, -0.4634]])\n",
            "Queries:\n",
            "tensor([[-0.1086,  0.0601],\n",
            "        [ 0.1191,  0.5777],\n",
            "        [ 0.1402,  0.5739],\n",
            "        [ 0.0321,  0.3643],\n",
            "        [ 0.4795,  0.3442],\n",
            "        [-0.1661,  0.4190]])\n",
            "Values:\n",
            "tensor([[-0.3956, -0.8296],\n",
            "        [-0.0180, -0.6356],\n",
            "        [-0.0220, -0.6500],\n",
            "        [ 0.0630, -0.2259],\n",
            "        [-0.0871, -0.7278],\n",
            "        [ 0.0965, -0.1153]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, one has 3 matrices containing 6 vectors (rows) each, all of dimension 2 (columns).\n",
        "\n",
        "## Attention scores\n",
        "\n",
        "Now, recalling the previous topic, one calculated the attention scores between the vector of interest $x^{(2)}$ and the other input vectors. Now, in this case one calculates those attention scores using the query and the keys for each input.\n",
        "\n",
        "Let's then calculate the attention scores using only $x^{(2)}$, this will give us a matrix of 6 attention scores containing the importance of $x^{(2)}$ with respect to the other input vectors."
      ],
      "metadata": {
        "id": "a7E9950XfqBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the attention scores matrix for x^2 only\n",
        "\n",
        "query_2 = queries[1] # x^2 as a query\n",
        "\n",
        "attention_scores_2 = query_2 @ keys.T\n",
        "print(attention_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyCMMSy8feRe",
        "outputId": "f6e9ae42-3c23-4348-8980-a4c7a4c44eaf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1749, -0.4604, -0.4479, -0.2973, -0.0958, -0.4332])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the 6 attention scores of $x^{(2)}$ with respecto to all other input vectors.\n",
        "\n",
        "Now, let's calculate the big attention scores matrix for all input vectors at once."
      ],
      "metadata": {
        "id": "Ox3qzt27vm1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores = queries @ keys.T # These are called omega\n",
        "print(attention_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YQAFLxwviLd",
        "outputId": "ebaef6e2-825a-436a-b5f3-a362e01be580"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1757,  0.2328,  0.2331,  0.1173,  0.1737,  0.1229],\n",
            "        [-0.1749, -0.4604, -0.4479, -0.2973, -0.0958, -0.4332],\n",
            "        [-0.2087, -0.5080, -0.4954, -0.3221, -0.1283, -0.4607],\n",
            "        [-0.0414, -0.1906, -0.1830, -0.1348,  0.0048, -0.2134],\n",
            "        [-0.7590, -1.2222, -1.2115, -0.6777, -0.6774, -0.8255],\n",
            "        [ 0.2780,  0.2521,  0.2592,  0.0939,  0.3140,  0.0366]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, each row corresponds to the attention scores of each input vector with respect to all the others.\n",
        "\n",
        "## Attention weights\n",
        "\n",
        "Now, as before, we'll calculate attention weights by normalizing the attention scores, but in this case, we scale the result by the square root of the dimension of the keys. This has to do with the sensitivity of the magnitude of the inputs. This then gives stability when learning. The square root has to do with the variance, as variance grows with dimensions, so dividing by the square root of the dimension keeps the variance close to 1."
      ],
      "metadata": {
        "id": "tDbsJStkwKqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dimension_keys = keys.shape[-1]\n",
        "attention_weights = torch.softmax(attention_scores / (dimension_keys ** 0.5), dim=-1)\n",
        "print(attention_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye8CcPoDwC3S",
        "outputId": "52157418-6cd5-41ba-b9bf-4156b53449b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1666, 0.1734, 0.1735, 0.1598, 0.1663, 0.1605],\n",
            "        [0.1835, 0.1500, 0.1513, 0.1683, 0.1941, 0.1529],\n",
            "        [0.1837, 0.1487, 0.1500, 0.1695, 0.1944, 0.1537],\n",
            "        [0.1767, 0.1590, 0.1599, 0.1654, 0.1826, 0.1565],\n",
            "        [0.1812, 0.1306, 0.1316, 0.1919, 0.1919, 0.1729],\n",
            "        [0.1750, 0.1718, 0.1727, 0.1536, 0.1795, 0.1475]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the results are normalized\n",
        "print(attention_weights.sum(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb--DucpyWAk",
        "outputId": "f4911116-2936-4d1f-823e-e6521c3732c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context vectors\n",
        "\n",
        "Finally, we are ready to calculate the context vectors just like we did with the simplified self-attention mechanism. In this case, once we have the attention weights, we multiply each attention weights corresponding to each input vector with the corresponding values, that is, the corresponding row in the values matrix. We sum them up and that gives the context vector.\n",
        "\n",
        "Recall that every row in the attention weights matrix and every row in the values matrix correspond to a particular input token.\n",
        "\n",
        "Also, recall that a particular context vector is obtained by scaling the original input vectors by the attention weights, then adding them up so that it is a regular vector addition."
      ],
      "metadata": {
        "id": "TwtL-csb4Y-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the context vector matrix for all tokens at once\n",
        "context_vectors = attention_weights @ values\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q2dv8loy8p_",
        "outputId": "19cb87b1-edde-454f-bf2b-b7bdfd5af348"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0617, -0.5368],\n",
            "        [-0.0702, -0.5428],\n",
            "        [-0.0700, -0.5419],\n",
            "        [-0.0666, -0.5399],\n",
            "        [-0.0648, -0.5218],\n",
            "        [-0.0678, -0.5489]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention class\n",
        "\n",
        "We will now implement a Python class to calculate self-attention."
      ],
      "metadata": {
        "id": "8F6xsYci8iWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttentionV1(nn.Module):\n",
        "    # Initialize the object with the number of input and output dimensions\n",
        "    def __init__(self, dimension_inputs, dimension_outputs):\n",
        "        super().__init__()\n",
        "        # Initialize the matrices\n",
        "        self.W_q = nn.Parameter(torch.randn(dimension_inputs, dimension_outputs), requires_grad=False)\n",
        "        self.W_k = nn.Parameter(torch.randn(dimension_inputs, dimension_outputs), requires_grad=False)\n",
        "        self.W_v = nn.Parameter(torch.randn(dimension_inputs, dimension_outputs), requires_grad=False)\n",
        "\n",
        "    # Method to calculate the context vector\n",
        "    def forward(self, input_vectors):\n",
        "        # Calculate the query, key, and value vectors\n",
        "        queries = input_vectors @ self.W_q\n",
        "        keys = input_vectors @ self.W_k\n",
        "        values = input_vectors @ self.W_v\n",
        "\n",
        "        # Calculate attention scores, that is, omega\n",
        "        attention_scores = queries @ keys.T\n",
        "\n",
        "        # Calculate attention weights\n",
        "        dimension_keys = keys.shape[-1]\n",
        "        attention_weights = torch.softmax(attention_scores / (dimension_keys ** 0.5), dim=-1)\n",
        "\n",
        "        # Calculate the context vectors\n",
        "        context_vectors = attention_weights @ values\n",
        "\n",
        "        # return context vectors\n",
        "        return context_vectors"
      ],
      "metadata": {
        "id": "FI-TuAIS6i4f"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_vuVjds-cFJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}