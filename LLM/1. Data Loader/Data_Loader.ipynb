{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvo863VMsczKOEo79XSSuf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/1.%20Data%20Loader/Data_Loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qx4zAQnUQbHv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/RCortez25/PhD/main/LLM/0.%20Tokenizer/the-veredict.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the downloaded content to a file\n",
        "with open(\"the-veredict.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "with open(\"the-veredict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "oTokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "encoded_text = oTokenizer.encode(raw_text)\n",
        "print(f\"Number of tokens: {len(encoded_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpHrBUf8amHp",
        "outputId": "94230545-c61d-48c8-bbad-8559f47c6c59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 5146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the first 50 tokens for demonstration purposes\n",
        "encoded_sample = encoded_text[50:]"
      ],
      "metadata": {
        "id": "AE8v-f-ucp76"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in order to create input-target pairs one creates two variables\n",
        "\n",
        "$x$: inputs\n",
        "\n",
        "$y$: targets\n",
        "\n",
        "For example:\n",
        "\n",
        "$x=[1,2,3,4]\\\\y=[2,3,4,5]$\n",
        "\n",
        "The `context_size` determines how many tokens to include."
      ],
      "metadata": {
        "id": "E52G6-32dKQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "\n",
        "# Creating input-target pair\n",
        "x = encoded_sample[:context_size]\n",
        "y = encoded_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COWCxnzmc1ck",
        "outputId": "918da1e1-3ea9-4fdb-8884-f1279042b4a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a simple example of how the LLM will process the text in the autoregressive scheme"
      ],
      "metadata": {
        "id": "QXVcEnCsfps1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = encoded_sample[:i]\n",
        "    desired = encoded_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9W6SLg6etBu",
        "outputId": "5db4e096-9273-4fe1-f309-7c2a14796b3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The same as before but with decoded text\n",
        "for i in range(1, context_size+1):\n",
        "    context = encoded_sample[:i]\n",
        "    desired = encoded_sample[i]\n",
        "\n",
        "    print(oTokenizer.decode(context), \"---->\", oTokenizer.decode([desired]))"
      ],
      "metadata": {
        "id": "vc8UpsjEgHwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bff6c95-5aa6-40f5-cd34-29d936919ca5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have to gather all input and output pairs into tensors for the training of the LLM. One will have then and $x$ tensor of inputs and a $y$ tensor of outputs.\n",
        "\n",
        "The first entry of the $x$ tensor will be paired with the first entry of the $y$ tensor. The second entry of $x$ will be paired with the second entry of $y$, and so on.\n",
        "\n",
        "Note: In the class below, the `stride` parameter controls the number of places the window is displaced. The context size refers to the width of the window and the stride indicates by how much the window is displaced. For example:\n",
        "\n",
        "\"In the heart of the city stood the old library\"\n",
        "\n",
        "`context_size=4, stride=1`\n",
        "\n",
        "Input of first batch = \"In the heart of\"\n",
        "\n",
        "Input of second batch = \"the heart of the\"\n",
        "\n",
        "`context_size=4, stride=4`\n",
        "\n",
        "Input of first batch = \"In the heart of\"\n",
        "\n",
        "Input of second batch = \"the city stood\"\n"
      ],
      "metadata": {
        "id": "6uG1aVYJpkoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDatasetV1(Dataset):\n",
        "    def __init__(self, text, tokenizer, context_size, stride):\n",
        "        # List to store all the successive encoded inputs\n",
        "        self.input_ids = []\n",
        "        # List to store all the successive encoded targets\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the input text\n",
        "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Start retrieving input-target pairs by looping through the encoded text\n",
        "        # This is accomplished by sliding the context window\n",
        "        # We loop from 0 to the entire dataset minus the context size because\n",
        "        # if one has 100 ids and the context size is 4, one stops at 95, so that\n",
        "        # the last target have tokens with indexes [96,97,98,99] as this is the\n",
        "        # last target and we don't have more inputs as the entire text is covered\n",
        "        for i in range(0, len(token_ids) - context_size, stride):\n",
        "            # Get the input slice from the encoded text\n",
        "            encoded_input = token_ids[i:i + context_size]\n",
        "            # Get the corresponding target slice to be paired with the input\n",
        "            encoded_target = token_ids[i + 1:i + context_size + 1]\n",
        "            # Append the input and target slices to their respective lists\n",
        "            # Important to make them tensors so that they're ready to work with\n",
        "            self.input_ids.append(torch.tensor(encoded_input))\n",
        "            self.target_ids.append(torch.tensor(encoded_target))\n",
        "\n",
        "    def __len__(self):\n",
        "        # Method to get the length of the created dataset\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Method to retrieve a single input:target item from the dataset\n",
        "        # idx is the index of the desired item\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "P1U8KJOzoWd4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll define a function that creates a data loader that helps us work with our dataset in an efficient manner"
      ],
      "metadata": {
        "id": "NEoacVDfUk_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(text, batch_size=4, context_size=256, stride=128,\n",
        "                         shuffle=True, drop_last=True, num_workers=0):\n",
        "    \"\"\"\n",
        "    Creates a PyTorch DataLoader for a text dataset.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text data.\n",
        "        batch_size (int): The number of samples per batch.\n",
        "        context_size (int): The size of the context window for each input sequence.\n",
        "        stride (int): The number of tokens to slide the context window by.\n",
        "        shuffle (bool, optional): Whether to shuffle the data. Defaults to True.\n",
        "        drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n",
        "        num_workers (int, optional): How many subprocesses to use for data loading. Defaults to 0.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.DataLoader: The DataLoader object.\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer\n",
        "    oTokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    # Create dataset\n",
        "    dataset = TextDatasetV1(text, oTokenizer, context_size, stride)\n",
        "    # Create the dataloader\n",
        "    oDataLoader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                             drop_last=drop_last, num_workers=num_workers)\n",
        "    return oDataLoader"
      ],
      "metadata": {
        "id": "_eQwx2fYweOA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "giBRwBfVWgoA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}