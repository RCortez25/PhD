{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0V/gEkj67d2ZBy9D+clQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/2.%20Token%20embeddings/Token_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "2YPozp5Ljpeo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy example\n",
        "\n",
        "Let's create a simple example to generate token embeddings."
      ],
      "metadata": {
        "id": "WIMKOqfgjSxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tCqLQgEHG87H"
      },
      "outputs": [],
      "source": [
        "# We have the text \"Quick fox is in the house\"\n",
        "\n",
        "# Say their token ids are [4, 0, 3, 2, 5, 1]\n",
        "\n",
        "# Create a tensor that stores the token ids [2, 3, 5, 1]\n",
        "# That is [in, is, the, house] to be transformed into token embeddings\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Say we have a vocabulary of 6 words, those given in the sentence above\n",
        "vocabulary_size = 6\n",
        "\n",
        "# We want to embbed the words into a 3-dimensional space. We can choose other\n",
        "# dimensionality, but we'll keep it 3-dimensional\n",
        "output_dimension = 3"
      ],
      "metadata": {
        "id": "P5SnNT3-kvMm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create an embedding layer for vectorize the words\n",
        "# PyTorch creates embedding layers suitable for the task of creating embedding\n",
        "# vectors\n",
        "embedding_layer = torch.nn.Embedding(vocabulary_size, output_dimension)\n",
        "\n",
        "# Take a look at the created tensor\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89ebOAP7lMyA",
        "outputId": "78f34b10-e784-4c3a-f159-5eff2a0ff42b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.9269,  1.4873, -0.4974],\n",
            "        [ 0.4396, -0.7581,  1.0783],\n",
            "        [ 0.8008,  1.6806,  0.3559],\n",
            "        [-0.6866,  0.6105,  1.3347],\n",
            "        [-0.2316,  0.0418, -0.2516],\n",
            "        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This created a matrix whose dimensions are `(vocabulary_size, output_dimension)`, where each row corresponds to a word in the vocabulary and each column to a direction in the hyperspace. The values are randomly initialized.\n",
        "\n",
        "Note that `requires_grad=True` indicates that the values are going to be adjusted via training the network.\n",
        "\n",
        "Let's get the vector of each word in the vocabulary."
      ],
      "metadata": {
        "id": "0U-SJtLLl7hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Vector for the word \"fox\": {embedding_layer.weight[0]}')\n",
        "print(f'Vector for the word \"house\": {embedding_layer.weight[1]}')\n",
        "print(f'Vector for the word \"in\": {embedding_layer.weight[2]}')\n",
        "print(f'Vector for the word \"is\": {embedding_layer.weight[3]}')\n",
        "print(f'Vector for the word \"quick\": {embedding_layer.weight[4]}')\n",
        "print(f'Vector for the word \"the\": {embedding_layer.weight[5]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy3BS76KlaB-",
        "outputId": "f0664935-e454-432c-dbfe-1d4dd98f98d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for the word \"fox\": tensor([ 1.9269,  1.4873, -0.4974], grad_fn=<SelectBackward0>)\n",
            "Vector for the word \"house\": tensor([ 0.4396, -0.7581,  1.0783], grad_fn=<SelectBackward0>)\n",
            "Vector for the word \"in\": tensor([0.8008, 1.6806, 0.3559], grad_fn=<SelectBackward0>)\n",
            "Vector for the word \"is\": tensor([-0.6866,  0.6105,  1.3347], grad_fn=<SelectBackward0>)\n",
            "Vector for the word \"quick\": tensor([-0.2316,  0.0418, -0.2516], grad_fn=<SelectBackward0>)\n",
            "Vector for the word \"the\": tensor([ 0.8599, -0.3097, -0.3957], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can obtain the vectors for the `input_ids` defined above."
      ],
      "metadata": {
        "id": "PxIJWT2io-tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcNDZzw7oAyf",
        "outputId": "520de38a-c8c0-401e-865f-b7574b8f1fca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8008,  1.6806,  0.3559],\n",
            "        [-0.6866,  0.6105,  1.3347],\n",
            "        [ 0.8599, -0.3097, -0.3957],\n",
            "        [ 0.4396, -0.7581,  1.0783]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, these values are random for now, they're optimized with the training of the LLM."
      ],
      "metadata": {
        "id": "Ke-nnLzMpTGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding layer = Linear layer\n",
        "\n",
        "The embedding layer creates a matrix that takes into account the size of the vocabulary and the desired dimensionality of the space one wants to embbed the words into."
      ],
      "metadata": {
        "id": "RIX9doXLrBI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a matrix for a vocabulary of 4 words to embedd them into a\n",
        "# 5 dimensional space\n",
        "\n",
        "embedding_layer = torch.nn.Embedding(4, 5)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9BCGIicpFQN",
        "outputId": "bac6205e-bb4d-47ba-bc16-b3edccbd0203"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.2234,  1.7174,  0.3189, -0.4245, -0.8140],\n",
            "        [-0.7360, -0.8371, -0.9224,  1.8113,  0.1606],\n",
            "        [ 0.3672,  0.1754, -1.1845,  1.3835, -1.2024],\n",
            "        [ 0.7078, -1.0759,  0.5357,  1.1754,  0.5612]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the vectors for the following ids\n",
        "input_ids = torch.tensor([0, 2])\n",
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYBPKxg7ra8U",
        "outputId": "5119640d-493b-46e1-aace-f9aea656727f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2234,  1.7174,  0.3189, -0.4245, -0.8140],\n",
            "        [ 0.3672,  0.1754, -1.1845,  1.3835, -1.2024]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the embedding layer is the same as a linear layer in a neural network. In this case, such a linear layer will have 4 inputs (akin to the vocabulary size) and 5 neurons (akin to the dimension of the space)."
      ],
      "metadata": {
        "id": "9fMd3KSsr-Op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, what's the benefit of using PyTorch's embedding layer? It is computationally more efficient."
      ],
      "metadata": {
        "id": "LCTUmOqetDxK"
      }
    }
  ]
}