{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvp8luaJlxKccEKwcuuCrT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/3.%20Positional%20embeddings/Positional_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "Sc9PeRV1xlZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use positional embeddings to account for the position of a token in a given sentence.\n",
        "\n",
        "**Absolute**: A unique embedding is added to the token embeddings. That is\n",
        "\n",
        "Token embeddings + Positional embeddings = Input embeddings\n",
        "\n",
        "For example:\n",
        "\n",
        "For the first token:\n",
        "$[1,2,3]+[1.1,1.2,1.3]=[1.1,3.1,4.3]$\n",
        "\n",
        "For the second token:\n",
        "$[4,5,6]+[2.1,2.2,2.3]=[6.1,7.2,8.3]$\n",
        "\n",
        "Depending on the position (first, second, etc.), a positional embedding is added. However, this method does not account for the relative position of each token, which can be of importance.\n",
        "\n",
        "**Relative**: In this case, the model learns the relationship between tokens taking into account their relative position in the sencente. The model generalizes better to sequences of varying lengths.\n",
        "\n",
        "Both methods are good and the use of one over the other depends on the problem and the nature of the data beign processed.\n",
        "\n",
        "**Absolute** is good when fixed order of tokens is important, for instance, qwhen generating a sequence (time series, physics, etc). GPT was trained using this, as well as the original transformer.\n",
        "\n",
        "**Relative** is good for language modeling over long sequences where the same phrase can appearin different parts of the sequence.\n",
        "\n",
        "This positional embeddings are also optimized during training.\n",
        "\n",
        "Now, let's look at a simple example."
      ],
      "metadata": {
        "id": "aSmy7W8iY3wM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_k1lX1wnWiZi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Using Data_loader notebook as a .py file in order to import the classes\n",
        "# This file is uploaded to the Colab workspace\n",
        "\n",
        "from data_loader import create_dataloader_v1\n",
        "\n",
        "# Import the text\n",
        "with open('/content/the-veredict.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for the embedding layer\n",
        "# In this case, we're going to map each token to a 256-dimensional space\n",
        "vocabulary_size = 50257\n",
        "output_dimension = 256\n",
        "\n",
        "# Create an embedding layer\n",
        "embedding_layer = torch.nn.Embedding(vocabulary_size, output_dimension)"
      ],
      "metadata": {
        "id": "yNJi53wThc4K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataloader to tokenize the text\n",
        "dataloader = create_dataloader_v1(\n",
        "    text=raw_text, batch_size=8, context_size=4, stride=4, shuffle=False)"
      ],
      "metadata": {
        "id": "lyAhtOC0krxf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the iterator\n",
        "iterator = iter(dataloader)\n",
        "\n",
        "# Check the first batch of tokens\n",
        "inputs, targets = next(iterator)\n",
        "\n",
        "print(f'Input tokens:\\n {inputs}')\n",
        "print(f'Target tokens:\\n {targets}')\n",
        "print()\n",
        "print(f'Shape of inputs: {inputs.shape}')\n",
        "print(f'Shape of targets: {targets.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aefqo83glJQR",
        "outputId": "3e3ae54c-c4eb-49b4-9d13-79e7c414c945"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "Target tokens:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n",
            "\n",
            "Shape of inputs: torch.Size([8, 4])\n",
            "Shape of targets: torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it is time to use the lookup table, i.e., the embedding matrix created by the embedding layer. This will map each token to the 256-dimensional space."
      ],
      "metadata": {
        "id": "B9fFXZGBwZ8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_vectors = embedding_layer(inputs)\n",
        "\n",
        "print(f'Shape of token vectors: {token_vectors.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0Ib2F5ol6K8",
        "outputId": "52a4b03e-7c46-4a7a-a258-fc1c984422a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of token vectors: torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we have mapped 8 examples of 4 tokens each, into a 256-dimensional space."
      ],
      "metadata": {
        "id": "pLePed-Gw6xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating positional embeddings\n",
        "\n",
        "We need to create another embedding layer for positional encoding, that is, we need to create another matrix of randomly initialized numbers. This mapping must be to the same dimension of the space, in this case, it must be a 256-dimensional space. Also, the number of rows must be the number of input tokens, in this case, 4, which is equal to the context size."
      ],
      "metadata": {
        "id": "J5dqzNzZxnkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "ouput_dimension = 256\n",
        "positional_embedding_layer = torch.nn.Embedding(context_size, ouput_dimension)"
      ],
      "metadata": {
        "id": "GqEODO4fw1pQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}