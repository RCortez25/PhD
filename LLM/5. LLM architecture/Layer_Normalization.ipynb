{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz/xpWjFJ9LdlUSkCPb36s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/5.%20LLM%20architecture/Layer_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example\n",
        "\n",
        "Due to the complexities of training (vanishing/exploding gradients), this layer improves the stability and efficiency of the NN training. It helps prevents problems with gradient and helps with convergence.\n",
        "\n",
        "This layer is created as a separate class because it doesn't happen only in the transformer but also outside of it. Normally, this layer is applied before and after the MultiHead attention module and before the final output layer, that is, inside and outside the transformer block.\n",
        "\n",
        "This layer normalizes the outputs, i.e., makes them have mean of 0 and a variance of 1. This speeds up convergence.\n",
        "\n",
        "Now, the `LayerNormPlaceholder` becomes `LayerNormalization`.\n",
        "\n",
        "Let's make a first simple example."
      ],
      "metadata": {
        "id": "6BoOW0oQfuw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5)\n",
        "batch_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VyuQUd_k-Wn",
        "outputId": "f7f897b0-7887-4ccb-a8a7-ff9baf294d80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
              "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sequential layer that maps the 5 inputs of each batch to a 6D-space\n",
        "# then uses a ReLU activation function\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "\n",
        "#Calculate the outputs\n",
        "outputs = layer(batch_example)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5-bBurtlSIp",
        "outputId": "29aed550-f3cc-4ecd-dfc7-c37a8130351e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1898, 0.4165, 0.0000, 0.5927, 0.0000],\n",
              "        [0.2917, 0.0000, 0.7161, 0.0000, 0.3986, 0.0000]],\n",
              "       grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check the mean and variance of the outputs before applying the layer normalization."
      ],
      "metadata": {
        "id": "oj_R_vW-moG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that this calculates the mean and variance of all the tensor\n",
        "outputs.mean()\n",
        "outputs.var()\n",
        "\n",
        "# However, we want the mean dna variance of each batch, that is, each row\n",
        "mean_batch = outputs.mean(dim=1, keepdim=True)\n",
        "var_batch = outputs.var(dim=1, keepdim=True)\n",
        "\n",
        "print(mean_batch)\n",
        "print(var_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DonmNnDOmhSR",
        "outputId": "1ab8d3e6-09ee-475a-9be0-aa09d99a2970"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1998],\n",
            "        [0.2344]], grad_fn=<MeanBackward1>)\n",
            "tensor([[0.0642],\n",
            "        [0.0854]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note what happens when we don't use keepdim=True\n",
        "\n",
        "mean_batch_ = outputs.mean(dim=1)\n",
        "var_batch_ = outputs.var(dim=1)\n",
        "\n",
        "print(mean_batch_)\n",
        "print(var_batch_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AGtAPOPnMT1",
        "outputId": "f4be2d33-63ce-48d8-f26c-1008b7559cd0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1998, 0.2344], grad_fn=<MeanBackward1>)\n",
            "tensor([0.0642, 0.0854], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So using `keepdim=True` makes the result have the same dimension as the inputs, which makes it readable as each row corresponds to each batch."
      ],
      "metadata": {
        "id": "ZDATnpCKnYl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of the layer"
      ],
      "metadata": {
        "id": "tN_eqJuDk8Wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUM3ROgbeD4o"
      },
      "outputs": [],
      "source": [
        "class LayerNormPlaceholder(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ]
    }
  ]
}