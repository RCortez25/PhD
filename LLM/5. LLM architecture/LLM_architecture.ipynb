{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbhDCYo4jlEUtyrTu9GLRD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/5.%20LLM%20architecture/LLM_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM architecture\n",
        "\n",
        "We're going to replicate GPT-2, with 124 million parameteres, whose weights are open source."
      ],
      "metadata": {
        "id": "yLLU1uXC7FfF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l_hfrcjppCAz"
      },
      "outputs": [],
      "source": [
        "# Configuration for our GPT model\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,        # Number of words/sub-words\n",
        "    \"context_length\": 1024,     # How many words used to predict the next word\n",
        "    \"embedding_dimension\": 768, # Tokens are projected into a 768-dimensional space\n",
        "    \"number_of_heads\": 12,      # This creates 12 query, key, and value matrices\n",
        "    \"number_of_layers\": 12,     # Number of transformer blocks\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"qkv_bias\": False}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT placeholder architecture\n",
        "\n",
        "We'll build a GPT placeholder architecture to gain intuition on how everything fits together. It will take the configuration we just outlined above."
      ],
      "metadata": {
        "id": "Ctj8lDkT-6aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPTModelPlaceholder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Initialize variables using the configuration dictionary\n",
        "        # Look up table from ids to embeddings\n",
        "        self.token_embedding_table = nn.Embedding(config[\"vocab_size\"], config[\"embedding_dimension\"])\n",
        "        # Look up table from position to position embedding\n",
        "        self.position_embedding_table = nn.Embedding(config[\"context_length\"], config[\"embedding_dimension\"])\n",
        "        self.dropout_embedding = nn.Dropout(config[\"dropout_rate\"])\n",
        "\n",
        "        # Placeholed for transformer blocks\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlockPlaceholder(config) for _ in range(config[\"number_of_layers\"])]\n",
        "        )\n",
        "\n",
        "        # Placeholder for LayerNorm\n",
        "        self.layer_norm = nn.LayerNorm(config[\"embedding_dimension\"])\n",
        "\n",
        "        # Output head\n",
        "        self.output_head = nn.Linear(config[\"embedding_dimension\"],\n",
        "                                     config[\"vocab_size\"],\n",
        "                                     bias=False)\n",
        "\n",
        "    # Method for accepting the inputs and make the transformations\n",
        "    # The inputs are fed into the model as tokens, that is, as IDs\n",
        "    def forward(self, inputs_ids):\n",
        "        # Obtain the size of the batch and the sequence length\n",
        "        batch_size, context_length = inputs_ids.shape\n",
        "\n",
        "        # Use the lookp table to obtain embeddings given the IDs\n",
        "        token_embeddings = self.token_embedding_table(inputs_ids)\n",
        "\n",
        "        # Obtain positional embeddings\n",
        "        # Create a range object whose length will be equal to the length of the\n",
        "        # inputs\n",
        "        range_object = torch.arange(context_length, device=inputs_ids.device)\n",
        "        # Use the object to use the lookup table for obtaining the positional\n",
        "        # embeddings corresponding to each position of each token\n",
        "        position_embeddings = self.position_embedding_table(range_object)\n",
        "\n",
        "        # Add the vector embeddings\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout_embedding(x)\n",
        "\n",
        "        # Now, the data is passed to transformers blocks\n",
        "        x = self.transformer_blocks(x)\n",
        "\n",
        "        # Data is passed through normalization layer\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Data is passed through the output head\n",
        "        logits = self.output_head(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "wElJ81CM74H9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer and Layer Normalization placeholders"
      ],
      "metadata": {
        "id": "T0RSLgrsXUOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlockPlaceholder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNormPlaceholder(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "cOw5HgF2XY9G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example\n",
        "\n",
        "First, let's create the batch of text to be used and tokenize it."
      ],
      "metadata": {
        "id": "wkkvDCVIMe9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Use GPT-2 encoded\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "\n",
        "# Text to be used in the example\n",
        "text1 = \"Every effort moves you\"\n",
        "text2 = \"Every day holds a\"\n",
        "\n",
        "# Obtain the IDs of each text\n",
        "text_1_tokenized = tokenizer.encode(text1)\n",
        "text_2_tokenized = tokenizer.encode(text2)\n",
        "\n",
        "batch.append(text_1_tokenized)\n",
        "batch.append(text_2_tokenized)\n",
        "\n",
        "batch = torch.tensor(batch)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRvr7S2DMgXw",
        "outputId": "c0793853-5ccf-42b2-ffad-cc7c7ba76602"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZFj6NFBNGAO",
        "outputId": "9a3b8de9-ffc2-4795-cbc9-876ff4f8c5bc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a GPT model and pass it this example."
      ],
      "metadata": {
        "id": "RBRn_hJuYU91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Create the model\n",
        "oGPT = GPTModelPlaceholder(GPT_CONFIG_124M)\n",
        "logits = oGPT(batch)\n",
        "print(f\"Logits' shape: {logits.shape}\")\n",
        "print(f\"\\nLogits: {logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7JhOT6jWLT-",
        "outputId": "207b5804-d822-4b2f-a127-2ba1e2718b2f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits' shape: torch.Size([2, 4, 50257])\n",
            "\n",
            "Logits: tensor([[[-0.7867,  0.2203, -0.4508,  ..., -0.9936, -0.1412, -0.2999],\n",
            "         [-0.0788,  0.3004, -0.2935,  ...,  0.1583,  0.8917,  0.8230],\n",
            "         [ 0.3708,  1.1126, -0.3226,  ...,  0.8023, -0.0038,  0.3935],\n",
            "         [ 0.0636,  1.0572, -0.2507,  ...,  0.7542, -0.0750, -0.6896]],\n",
            "\n",
            "        [[-0.7208,  0.1351, -0.6014,  ..., -1.0272,  0.1729, -0.2920],\n",
            "         [-0.5938,  0.4453, -0.0059,  ...,  0.3414,  0.0572,  1.0986],\n",
            "         [ 0.2675,  0.8407, -0.4476,  ..., -0.0181, -0.1090,  0.2541],\n",
            "         [-0.1035, -0.5901, -0.3932,  ...,  1.4022, -0.3188,  0.1304]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, we have two batches of inputs, each containing 4 inputs and the corresponding probabilities for all the 50257 tokens in the vocabulary."
      ],
      "metadata": {
        "id": "uVUKxCGhY8ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logits\n",
        "\n",
        "The final output of the GPT class has as its output a tensor with rows equal to the number of tokens fed into it. For the example \"Every effort moves you\", since these are 4 tokens then the output will have 4 rows. Now, this output will have a number of columns equal to the size of the vocabulary and each entry will contain a probability. For example, if the vocabulary size is 50257 then the output will have:\n",
        "\n",
        "* 4 rows\n",
        "* 50257 columns\n",
        "\n",
        "where each entry will correspond to a probability, associated with each word in the vocabulary, that corresponds to the probabilities of each word in the vocabulary to be the next token.\n",
        "\n",
        "For example, for the first row corresponding to \"Every\", the 50257 columns will contain the probabilities for each token in the vocabulary to be the next word after \"Every\". For the next row one will have \"Every effort\", and the 50257 columns will contain the probabilities for each token in the vocabulary to be the next word after \"Every effort\", and so on.\n",
        "\n",
        "These entries are termed **logits**.\n",
        "\n",
        "That's why in the code, for the `output_head` one has the dimensions equal to the number of tokens and the vocabulary size."
      ],
      "metadata": {
        "id": "RtgO1wdOTNEh"
      }
    }
  ]
}