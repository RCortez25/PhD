{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7V9dn+9C+48fX/zbozhnB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/6.%20Layer%20Normalization/Layer_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example\n",
        "\n",
        "Due to the complexities of training (vanishing/exploding gradients), this layer improves the stability and efficiency of the NN training. It helps prevents problems with gradient and helps with convergence.\n",
        "\n",
        "This layer is created as a separate class because it doesn't happen only in the transformer but also outside of it. Normally, this layer is applied before and after the MultiHead attention module and before the final output layer, that is, inside and outside the transformer block.\n",
        "\n",
        "This layer normalizes the outputs, i.e., makes them have mean of 0 and a variance of 1. This speeds up convergence.\n",
        "\n",
        "Now, the `LayerNormPlaceholder` becomes `LayerNormalization`.\n",
        "\n",
        "Let's make a first simple example."
      ],
      "metadata": {
        "id": "6BoOW0oQfuw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5)\n",
        "batch_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VyuQUd_k-Wn",
        "outputId": "f7f897b0-7887-4ccb-a8a7-ff9baf294d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
              "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sequential layer that maps the 5 inputs of each batch to a 6D-space\n",
        "# then uses a ReLU activation function\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "\n",
        "#Calculate the outputs\n",
        "outputs = layer(batch_example)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5-bBurtlSIp",
        "outputId": "29aed550-f3cc-4ecd-dfc7-c37a8130351e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1898, 0.4165, 0.0000, 0.5927, 0.0000],\n",
              "        [0.2917, 0.0000, 0.7161, 0.0000, 0.3986, 0.0000]],\n",
              "       grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check the mean and variance of the outputs before applying the layer normalization."
      ],
      "metadata": {
        "id": "oj_R_vW-moG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that this calculates the mean and variance of all the tensor\n",
        "outputs.mean()\n",
        "outputs.var()\n",
        "\n",
        "# However, we want the mean dna variance of each batch, that is, each row\n",
        "mean_batch = outputs.mean(dim=1, keepdim=True)\n",
        "var_batch = outputs.var(dim=1, keepdim=True)\n",
        "\n",
        "print(mean_batch)\n",
        "print(var_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DonmNnDOmhSR",
        "outputId": "1ab8d3e6-09ee-475a-9be0-aa09d99a2970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1998],\n",
            "        [0.2344]], grad_fn=<MeanBackward1>)\n",
            "tensor([[0.0642],\n",
            "        [0.0854]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note what happens when we don't use keepdim=True\n",
        "\n",
        "mean_batch_ = outputs.mean(dim=1)\n",
        "var_batch_ = outputs.var(dim=1)\n",
        "\n",
        "print(mean_batch_)\n",
        "print(var_batch_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AGtAPOPnMT1",
        "outputId": "f4be2d33-63ce-48d8-f26c-1008b7559cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1998, 0.2344], grad_fn=<MeanBackward1>)\n",
            "tensor([0.0642, 0.0854], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So using `keepdim=True` makes the result have the same dimension as the inputs, which makes it readable as each row corresponds to each batch.\n",
        "\n",
        "Now, let's apply normalization to the outputs."
      ],
      "metadata": {
        "id": "ZDATnpCKnYl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_outputs = (outputs - mean_batch) / torch.sqrt(var_batch + 1e-5)\n",
        "normalized_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT9fJfPPn7IQ",
        "outputId": "eae13a3a-4c4d-479f-8b6f-82ffbe43150c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7884, -0.0395,  0.8549, -0.7884,  1.5499, -0.7884],\n",
              "        [ 0.1960, -0.8019,  1.6481, -0.8019,  0.5618, -0.8019]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check meand and variance\n",
        "print(f'Means:\\n {normalized_outputs.mean(dim=1, keepdim=True)}')\n",
        "print(f'Variances:\\n {normalized_outputs.var(dim=1, keepdim=True)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UEwNt4sn--2",
        "outputId": "49f04950-8f42-4e5e-bcd0-c541ef1d277e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Means:\n",
            " tensor([[-6.9539e-08],\n",
            "        [-1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "Variances:\n",
            " tensor([[0.9998],\n",
            "        [0.9999]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results now have a mean of 0 and a variance of 1 for each batch. If we turn off scientific notation the results are even clearer."
      ],
      "metadata": {
        "id": "LFiwAiTgof5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False)\n",
        "print(f'Means:\\n {normalized_outputs.mean(dim=1, keepdim=True)}')\n",
        "print(f'Variances:\\n {normalized_outputs.var(dim=1, keepdim=True)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhuB5JFno0WH",
        "outputId": "5c380dad-eb0b-4012-cbb6-e6391e5c2e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Means:\n",
            " tensor([[    -0.0000],\n",
            "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variances:\n",
            " tensor([[0.9998],\n",
            "        [0.9999]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of the layer"
      ],
      "metadata": {
        "id": "tN_eqJuDk8Wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUM3ROgbeD4o"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, embedding_dimension):\n",
        "        super().__init__()\n",
        "        # Small number to prevent divison by zero\n",
        "        self.epsilon = 1e-5\n",
        "        # gamma scales\n",
        "        self.gamma = nn.Parameter(torch.ones(embedding_dimension))\n",
        "        # beta shifts\n",
        "        self.beta = nn.Parameter(torch.zeros(embedding_dimension))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        normalized_x = (x - mean) / torch.sqrt(std + self.epsilon)\n",
        "        return self.gamma * normalized_x + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code, $\\gamma$ and $\\beta$ are traininable parameters that the model learns that help improve the model's performance.\n",
        "\n",
        "The unbiased variance refers to the fact that one divides by $n-1$ instead of $n$.\n",
        "\n",
        "Now, let's use the class to process the example above."
      ],
      "metadata": {
        "id": "OfgumMDWqngN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oLayerNormalization = LayerNormalization(embedding_dimension=5)\n",
        "outputs_layerNormalization = oLayerNormalization(batch_example)\n",
        "print(f'Outputs:\\n {outputs_layerNormalization}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7kk5iNBpygS",
        "outputId": "ca9125fb-dd1b-4d25-e800-566909b187a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs:\n",
            " tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
            "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = outputs_layerNormalization.mean(dim=1, keepdim=True)\n",
        "var = outputs_layerNormalization.var(dim=1, keepdim=True, unbiased=False)\n",
        "print(f'Means:\\n {mean}')\n",
        "print(f'Variances:\\n {var}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMuHuLrdsIZB",
        "outputId": "b030e7f0-afc3-46d9-8a0d-03ed0781efa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Means:\n",
            " tensor([[    -0.0000],\n",
            "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variances:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    }
  ]
}