{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0NjvKgw581wopp7LDTZzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/8.%20Shortcut%20connections/Shortcut_connections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shortcut connections\n",
        "\n",
        "Shortcut connections were introduced to solve the problem of vanishing gradients, in which gradients become smaller and smaller as they propagate backward so it's difficult to train earlier layers.\n",
        "\n",
        "These connections create an alternative path for the gradient to flow by skipping one or more layers. This is achieved by adding the output of one layer to the output of a latter layer.\n",
        "\n",
        "Let's create a NN to see the effect of shortcut connections."
      ],
      "metadata": {
        "id": "sOyBMQWK5rmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "        return result"
      ],
      "metadata": {
        "id": "ddR0YqJ1_UDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rckh1R355R7r"
      },
      "outputs": [],
      "source": [
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, layer_sizes, use_shortcut):\n",
        "        super().__init__()\n",
        "        self.use_shortcut = use_shortcut\n",
        "        self.layers = nn.ModuleList([\n",
        "            for i in range(len(layer_sizes) - 1):\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(layer_sizes[i], layer_sizes[i + 1]),\n",
        "                    GELU()),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            # Compute the output of the current layer\n",
        "            output = layer(x)\n",
        "            # Check if shorcut can be applied\n",
        "            if self.use_shortcut and x.shape == output.shape:\n",
        "                # Apply shortcut\n",
        "                x = x + output\n",
        "            else\n",
        "                x = output\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "e1mH0DqYBP4B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PSUlTBqxBQkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}