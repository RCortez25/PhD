{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8fjGx/UFeJwxXW2XnM0Mo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/0.%20Tokenizer/Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "rmaSvCN4vo22"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Creating tokens"
      ],
      "metadata": {
        "id": "ejIU1SoQ81st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the sample text to work with\n",
        "with open('/content/the-veredict.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# Printing a sample\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO8F7SC68vSI",
        "outputId": "dfd37395-2b86-41ed-e13f-bd8c86be8c47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text to split it into tokens\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKI5InlWPCb6",
        "outputId": "97f6cbde-019c-47c5-debf-a5a23f18d5fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove whitespaces\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIPGbcdoPgt7",
        "outputId": "48df8e96-6fc1-44c1-c7f0-99131796ca05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is now tokenized"
      ],
      "metadata": {
        "id": "BKEKCjleQg9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Creating the vocabulary"
      ],
      "metadata": {
        "id": "TqF-iWmKCj0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a list of unique words\n",
        "unique_words = sorted(set(preprocessed))\n",
        "print(unique_words[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdcKGGaJQb_S",
        "outputId": "2489c6b6-9833-40c6-d9aa-1571262b38cf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, we now create the vocabulary. In this case, we map each character/word to its index in the list of unique words."
      ],
      "metadata": {
        "id": "RY_XHboIRDo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of token:index pairs\n",
        "vocabulary = {token:index for index,token in enumerate(unique_words)}\n",
        "\n",
        "# Check the first 15 elements in the vocabulary\n",
        "for token, index in vocabulary.items():\n",
        "    print(f'{token}: {index}')\n",
        "    if index == 15:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYCTqh7WRWfw",
        "outputId": "2c02a1e3-4d82-4ab8-9312-59b653bee936"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!: 0\n",
            "\": 1\n",
            "': 2\n",
            "(: 3\n",
            "): 4\n",
            ",: 5\n",
            "--: 6\n",
            ".: 7\n",
            ":: 8\n",
            ";: 9\n",
            "?: 10\n",
            "A: 11\n",
            "Ah: 12\n",
            "Among: 13\n",
            "And: 14\n",
            "Are: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the tokenizer\n",
        "\n",
        "Now that we have the vocabulary of tokens, we can then create a class that maps tokens to its numeric representation, i.e., its index in the vocabulary. The class will also decode a text, that is, convert numbers to tokens in the vocabulary. We call this class a **tokenizer**."
      ],
      "metadata": {
        "id": "WYiN8gwlQcy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"\n",
        "        vocab: Dictionary of word:index pairs\n",
        "        \"\"\"\n",
        "        self.str_to_int = vocab\n",
        "        # Create a dictionary as a lookup table\n",
        "        # Contains index:word pairs\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a given text into numbers using the vocabulary\n",
        "        text: Any text to be encoded\n",
        "        \"\"\"\n",
        "        # Preprocess the input text by splitting it\n",
        "        preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Remove whitespace if present for every splitted word\n",
        "        preprocessed_text = [\n",
        "            element.strip() for element in preprocessed_text if element.strip()\n",
        "        ]\n",
        "\n",
        "        # Create the number ids for each word using the vocabulary\n",
        "        # This creates a list of pure numbers which is the encoded text\n",
        "        ids = [self.str_to_int[element] for element in preprocessed_text]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Decodes a given list of numbers into text\n",
        "        ids: List of numbers to be decoded\n",
        "        \"\"\"\n",
        "        # Decoding the numbers into text\n",
        "        decoded_words = [self.int_to_str[element] for element in ids]\n",
        "        # Joins all the words in the list decoded_words inserting a blank space\n",
        "        # between them\n",
        "        decoded_text = ' '.join(decoded_words)\n",
        "        # Replace blank spaces before punctuations. \"Word1 , word2 . Hello\"\n",
        "        # Becomes \"Word1, word2. Hello\"\n",
        "        decoded_text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', decoded_text)\n",
        "        return decoded_text"
      ],
      "metadata": {
        "id": "mejevXsfrfOv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this version of the tokenizer."
      ],
      "metadata": {
        "id": "T4hTbq0kSomi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the tokenizer\n",
        "oTokenizerV1 = SimpleTokenizerV1(vocabulary)\n",
        "\n",
        "# Create a sample text to test the tokenizer\n",
        "sample_text = \"\"\"\"It's the last he painted, you know,\"\n",
        "                  Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "# Encode the sample text\n",
        "ids = oTokenizerV1.encode(sample_text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "zmrmHNC1vp-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08436a93-d608-435b-acc5-e70870890e80"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the ids of the sample text\n",
        "decoded_text = oTokenizerV1.decode(ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib-pd-VqS2ec",
        "outputId": "b3e76d4d-5c2a-4f7c-ed13-3d872263f25a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer works just fine. However, this is limited to words in the vocabulary. To account for possible tokens not contained in the vocabulary one must include some special tokens."
      ],
      "metadata": {
        "id": "1XiX7BqaT6W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test a new sample text that contains the word \"Hello\" not contained in the\n",
        "# vocabulary\n",
        "\n",
        "sample_text2 = \"Hello, how are you?\"\n",
        "ids = oTokenizerV1.encode(sample_text2)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "fh-DeJ-ITVcw",
        "outputId": "7caad5f0-b690-40b6-8b1b-97bfbe51b473"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-1587936696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_text2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, how are you?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moTokenizerV1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-2536361009.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Create the number ids for each word using the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# This creates a list of pure numbers which is the encoded text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-2536361009.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Create the number ids for each word using the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# This creates a list of pure numbers which is the encoded text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the special tokens <|unk|> for unknown tokens\n",
        "# and <|endoftext|> for separating different texts\n",
        "\n",
        "vocabulary['<|endoftext|>'] = len(vocabulary)\n",
        "vocabulary['<|unk|>'] = len(vocabulary)\n",
        "\n",
        "# Checking the last 4 tokens in the vocabulary\n",
        "for token, index in list(vocabulary.items())[-4:]:\n",
        "    print(f'{token}: {index}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiOl8m_KUiwq",
        "outputId": "d9f8bc60-c8ac-4014-8d53-81f9d56dd5bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your: 1128\n",
            "yourself: 1129\n",
            "<|endoftext|>: 1130\n",
            "<|unk|>: 1131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we modify SimpleTokenizerV1 to include the new tokens"
      ],
      "metadata": {
        "id": "O74KeunBW5ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"\n",
        "        vocab: Dictionary of word:index pairs\n",
        "        \"\"\"\n",
        "        self.str_to_int = vocab\n",
        "        # Create a dictionary as a lookup table\n",
        "        # Contains index:word pairs\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a given text into numbers using the vocabulary\n",
        "        text: Any text to be encoded\n",
        "        \"\"\"\n",
        "        # Preprocess the input text by splitting it\n",
        "        preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Remove whitespace if present for every splitted word\n",
        "        preprocessed_text = [\n",
        "            element.strip() for element in preprocessed_text if element.strip()\n",
        "        ]\n",
        "\n",
        "        # Add the special token <|unk|> if a token is not present in the\n",
        "        # vocabulary\n",
        "\n",
        "        ids = [\n",
        "            self.str_to_int.get(element, self.str_to_int['<|unk|>'])\n",
        "            for element in preprocessed_text\n",
        "        ]\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Decodes a given list of numbers into text\n",
        "        ids: List of numbers to be decoded\n",
        "        \"\"\"\n",
        "        # Decoding the numbers into text\n",
        "        decoded_words = [self.int_to_str[element] for element in ids]\n",
        "        # Joins all the words in the list decoded_words inserting a blank space\n",
        "        # between them\n",
        "        decoded_text = ' '.join(decoded_words)\n",
        "        # Replace blank spaces before punctuations. \"Word1 , word2 . Hello\"\n",
        "        # Becomes \"Word1, word2. Hello\"\n",
        "        decoded_text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', decoded_text)\n",
        "        return decoded_text"
      ],
      "metadata": {
        "id": "EMLN8JSVXB1w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this new version of the tokenizer"
      ],
      "metadata": {
        "id": "ceCRBEwsXn1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oTokenizerV2 = SimpleTokenizerV2(vocabulary)\n",
        "\n",
        "sample_text2 = \"Hello, how are you?\"\n",
        "ids = oTokenizerV2.encode(sample_text2)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBERhLatXloV",
        "outputId": "15ee11b2-25da-43cd-afa7-2d27d1cb18ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 560, 169, 1126, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text2 = oTokenizerV2.decode(ids)\n",
        "print(decoded_text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mMFHp-3XwNm",
        "outputId": "07b03988-873d-439e-cd96-cda10aa59382"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's try with two texts\n",
        "sample_text3 = \"I like rainy days\"\n",
        "\n",
        "two_sample_texts = \" <|endoftext|> \".join([sample_text2, sample_text3])\n",
        "print(two_sample_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFakCXQoX4Pa",
        "outputId": "6202e2df-390a-41ea-ed22-24138532cc50"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you? <|endoftext|> I like rainy days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the joined texts\n",
        "ids = oTokenizerV2.encode(two_sample_texts)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UQnP9t8YOj8",
        "outputId": "24046ab4-c1df-471d-b61e-a7b8a930536a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 560, 169, 1126, 10, 1130, 53, 628, 1131, 316]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the ids of the joined sample texts\n",
        "decoded_joined_texts = oTokenizerV2.decode(ids)\n",
        "print(decoded_joined_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfXrkhatYUgv",
        "outputId": "3539131d-9f40-41c3-9f7f-256d2eed505a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, how are you? <|endoftext|> I like <|unk|> days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte pair encoding"
      ],
      "metadata": {
        "id": "5z1kKm94YugU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oMf7qugnYcQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}