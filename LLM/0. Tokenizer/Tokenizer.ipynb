{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJt6Zkajdh5C2E6KDQlO5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/PhD/blob/main/LLM/0.%20Tokenizer/Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "rmaSvCN4vo22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Creating tokens"
      ],
      "metadata": {
        "id": "ejIU1SoQ81st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the sample text to work with\n",
        "with open('/content/the-veredict.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# Printing a sample\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO8F7SC68vSI",
        "outputId": "dfd37395-2b86-41ed-e13f-bd8c86be8c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text to split it into tokens\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKI5InlWPCb6",
        "outputId": "97f6cbde-019c-47c5-debf-a5a23f18d5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove whitespaces\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIPGbcdoPgt7",
        "outputId": "48df8e96-6fc1-44c1-c7f0-99131796ca05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is now tokenized"
      ],
      "metadata": {
        "id": "BKEKCjleQg9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Creating the vocabulary"
      ],
      "metadata": {
        "id": "TqF-iWmKCj0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a list of unique words\n",
        "unique_words = sorted(set(preprocessed))\n",
        "print(unique_words[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdcKGGaJQb_S",
        "outputId": "2489c6b6-9833-40c6-d9aa-1571262b38cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, we now create the vocabulary. In this case, we map each character/word to its index in the list of unique words."
      ],
      "metadata": {
        "id": "RY_XHboIRDo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of token:index pairs\n",
        "vocabulary = {token:index for index,token in enumerate(unique_words)}\n",
        "\n",
        "# Check the first 15 elements in the vocabulary\n",
        "for token, index in vocabulary.items():\n",
        "    print(f'{token}: {index}')\n",
        "    if index == 15:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYCTqh7WRWfw",
        "outputId": "2c02a1e3-4d82-4ab8-9312-59b653bee936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!: 0\n",
            "\": 1\n",
            "': 2\n",
            "(: 3\n",
            "): 4\n",
            ",: 5\n",
            "--: 6\n",
            ".: 7\n",
            ":: 8\n",
            ";: 9\n",
            "?: 10\n",
            "A: 11\n",
            "Ah: 12\n",
            "Among: 13\n",
            "And: 14\n",
            "Are: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the tokenizer\n",
        "\n",
        "Now that we have the vocabulary of tokens, we can then create a class that maps tokens to its numeric representation, i.e., its index in the vocabulary. The class will also decode a text, that is, convert numbers to tokens in the vocabulary. We call this class a **tokenizer**."
      ],
      "metadata": {
        "id": "WYiN8gwlQcy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"\n",
        "        vocab: Dictionary of word:index pairs\n",
        "        \"\"\"\n",
        "        self.str_to_int = vocab\n",
        "        # Create a dictionary as a lookup table\n",
        "        # Contains index:word pairs\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a given text into numbers using the vocabulary\n",
        "        text: Any text to be encoded\n",
        "        \"\"\"\n",
        "        # Preprocess the input text by splitting it\n",
        "        preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Remove whitespace if present for every splitted word\n",
        "        preprocessed_text = [\n",
        "            element.strip() for element in preprocessed_text if element.strip()\n",
        "        ]\n",
        "\n",
        "        # Create the number ids for each word using the vocabulary\n",
        "        # This creates a list of pure numbers which is the encoded text\n",
        "        ids = [self.str_to_int[element] for element in preprocessed_text]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Decodes a given list of numbers into text\n",
        "        ids: List of numbers to be decoded\n",
        "        \"\"\"\n",
        "        # Decoding the numbers into text\n",
        "        decoded_words = [self.int_to_str[element] for element in ids]\n",
        "        # Joins all the words in the list decoded_words inserting a blank space\n",
        "        # between them\n",
        "        decoded_text = ' '.join(decoded_words)\n",
        "        # Replace blank spaces before punctuations. \"Word1 , word2 . Hello\"\n",
        "        # Becomes \"Word1, word2. Hello\"\n",
        "        decoded_text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', decoded_text)\n",
        "        return decoded_text"
      ],
      "metadata": {
        "id": "mejevXsfrfOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this version of the tokenizer."
      ],
      "metadata": {
        "id": "T4hTbq0kSomi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the tokenizer\n",
        "oTokenizerV1 = SimpleTokenizerV1(vocabulary)\n",
        "\n",
        "# Create a sample text to test the tokenizer\n",
        "sample_text = \"\"\"\"It's the last he painted, you know,\"\n",
        "                  Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "# Encode the sample text\n",
        "ids = oTokenizerV1.encode(sample_text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "zmrmHNC1vp-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08436a93-d608-435b-acc5-e70870890e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the ids of the sample text\n",
        "decoded_text = oTokenizerV1.decode(ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib-pd-VqS2ec",
        "outputId": "b3e76d4d-5c2a-4f7c-ed13-3d872263f25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer works just fine. However, this is limited to words in the vocabulary. To account for possible tokens not contained in the vocabulary one must include some special tokens."
      ],
      "metadata": {
        "id": "1XiX7BqaT6W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test a new sample text that contains the word \"Hello\" not contained in the\n",
        "# vocabulary\n",
        "\n",
        "sample_text2 = \"Hello, how are you?\"\n",
        "ids = oTokenizerV1.encode(sample_text2)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "fh-DeJ-ITVcw",
        "outputId": "7caad5f0-b690-40b6-8b1b-97bfbe51b473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-1587936696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_text2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, how are you?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moTokenizerV1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-2536361009.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Create the number ids for each word using the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# This creates a list of pure numbers which is the encoded text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-2536361009.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Create the number ids for each word using the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# This creates a list of pure numbers which is the encoded text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the special tokens <|unk|> for unknown tokens\n",
        "# and <|endoftext|> for separating different texts\n",
        "\n",
        "vocabulary['<|endoftext|>'] = len(vocabulary)\n",
        "vocabulary['<|unk|>'] = len(vocabulary)\n",
        "\n",
        "# Checking the last 4 tokens in the vocabulary\n",
        "for token, index in list(vocabulary.items())[-4:]:\n",
        "    print(f'{token}: {index}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiOl8m_KUiwq",
        "outputId": "d9f8bc60-c8ac-4014-8d53-81f9d56dd5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your: 1128\n",
            "yourself: 1129\n",
            "<|endoftext|>: 1130\n",
            "<|unk|>: 1131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we modify SimpleTokenizerV1 to include the new tokens"
      ],
      "metadata": {
        "id": "O74KeunBW5ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"\n",
        "        vocab: Dictionary of word:index pairs\n",
        "        \"\"\"\n",
        "        self.str_to_int = vocab\n",
        "        # Create a dictionary as a lookup table\n",
        "        # Contains index:word pairs\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a given text into numbers using the vocabulary\n",
        "        text: Any text to be encoded\n",
        "        \"\"\"\n",
        "        # Preprocess the input text by splitting it\n",
        "        preprocessed_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Remove whitespace if present for every splitted word\n",
        "        preprocessed_text = [\n",
        "            element.strip() for element in preprocessed_text if element.strip()\n",
        "        ]\n",
        "\n",
        "        # Add the special token <|unk|> if a token is not present in the\n",
        "        # vocabulary\n",
        "\n",
        "        ids = [\n",
        "            self.str_to_int.get(element, self.str_to_int['<|unk|>'])\n",
        "            for element in preprocessed_text\n",
        "        ]\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Decodes a given list of numbers into text\n",
        "        ids: List of numbers to be decoded\n",
        "        \"\"\"\n",
        "        # Decoding the numbers into text\n",
        "        decoded_words = [self.int_to_str[element] for element in ids]\n",
        "        # Joins all the words in the list decoded_words inserting a blank space\n",
        "        # between them\n",
        "        decoded_text = ' '.join(decoded_words)\n",
        "        # Replace blank spaces before punctuations. \"Word1 , word2 . Hello\"\n",
        "        # Becomes \"Word1, word2. Hello\"\n",
        "        decoded_text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', decoded_text)\n",
        "        return decoded_text"
      ],
      "metadata": {
        "id": "EMLN8JSVXB1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this new version of the tokenizer"
      ],
      "metadata": {
        "id": "ceCRBEwsXn1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oTokenizerV2 = SimpleTokenizerV2(vocabulary)\n",
        "\n",
        "sample_text2 = \"Hello, how are you?\"\n",
        "ids = oTokenizerV2.encode(sample_text2)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBERhLatXloV",
        "outputId": "15ee11b2-25da-43cd-afa7-2d27d1cb18ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 560, 169, 1126, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text2 = oTokenizerV2.decode(ids)\n",
        "print(decoded_text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mMFHp-3XwNm",
        "outputId": "07b03988-873d-439e-cd96-cda10aa59382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's try with two texts\n",
        "sample_text3 = \"I like rainy days\"\n",
        "\n",
        "two_sample_texts = \" <|endoftext|> \".join([sample_text2, sample_text3])\n",
        "print(two_sample_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFakCXQoX4Pa",
        "outputId": "6202e2df-390a-41ea-ed22-24138532cc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you? <|endoftext|> I like rainy days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the joined texts\n",
        "ids = oTokenizerV2.encode(two_sample_texts)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UQnP9t8YOj8",
        "outputId": "24046ab4-c1df-471d-b61e-a7b8a930536a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 560, 169, 1126, 10, 1130, 53, 628, 1131, 316]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the ids of the joined sample texts\n",
        "decoded_joined_texts = oTokenizerV2.decode(ids)\n",
        "print(decoded_joined_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfXrkhatYUgv",
        "outputId": "3539131d-9f40-41c3-9f7f-256d2eed505a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, how are you? <|endoftext|> I like <|unk|> days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte pair encoding"
      ],
      "metadata": {
        "id": "5z1kKm94YugU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the **tiktoken** tokenizer which is a BPE algorithm utilized in GPT models."
      ],
      "metadata": {
        "id": "8KRbuHXvEbVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MQF1Te6D4cw",
        "outputId": "1d80e425-6449-401f-dce0-f69d7e0dbf7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Instantiate the tokenizer using the gpt2 model for pedagogic reasons\n",
        "oTokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "RCXzVUMtEnjb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "aEEtw2-xGS9B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b5fbcaf",
        "outputId": "507f8518-77bd-4c9b-cd05-5ea1cda91641"
      },
      "source": [
        "# Note that one can also use another most advanced models\n",
        "\n",
        "# Get the encoding for the gpt-4 model\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "# Sample text to encode\n",
        "sample_text = \"This is an example sentence to encode.\"\n",
        "\n",
        "# Encode the sample text\n",
        "encoded_ids = enc.encode(sample_text)\n",
        "\n",
        "print(f\"Original text: '{sample_text}'\")\n",
        "print(f\"Encoded IDs: {encoded_ids}\")\n",
        "\n",
        "# Decode the IDs back to text\n",
        "decoded_text = enc.decode(encoded_ids)\n",
        "print(f\"Decoded text: '{decoded_text}'\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: 'This is an example sentence to encode.'\n",
            "Encoded IDs: [2028, 374, 459, 3187, 11914, 311, 16559, 13]\n",
            "Decoded text: 'This is an example sentence to encode.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "bnxbWpN5GUfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we can use this tokenizer the same way as our V1 and V2 tokenizers\n",
        "sample_text = \"Hello, how are you? <|endoftext|> I like rainy days\"\n",
        "ids = oTokenizer.encode(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "arbgq2CXGJA-",
        "outputId": "eeed899a-a8a4-4102-bcb4-2389c8cfa7cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-1677525787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now, we can use this tokenizer the same way as our V1 and V2 tokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, how are you? <|endoftext|> I like rainy days\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tiktoken/core.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mdisallowed_special\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisallowed_special\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0m_special_token_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisallowed_special\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mraise_disallowed_special_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tiktoken/core.py\u001b[0m in \u001b[0;36mraise_disallowed_special_token\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_disallowed_special_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;34mf\"Encountered text corresponding to disallowed special token {token!r}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;34m\"If you want this text to be encoded as a special token, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to avoid the error, one must include the usage of special tokens\n",
        "ids = oTokenizer.encode(sample_text, allowed_special={\"<|endoftext|>\"})\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRe1VUNhGq7c",
        "outputId": "cf862cbc-51a9-43d7-e3d4-c10673112589"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 703, 389, 345, 30, 220, 50256, 314, 588, 37259, 1528]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how this tokenizer handles a slightly more complicated example in which there's a typo"
      ],
      "metadata": {
        "id": "3W5PVWj4HbKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Hello, how are you? <|endoftext|> I like rainydays\"\n",
        "ids = oTokenizer.encode(sample_text, allowed_special={\"<|endoftext|>\"})\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcQv85A6G945",
        "outputId": "b7f4abf7-6b7c-4b63-e6ce-7b5114407b55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 703, 389, 345, 30, 220, 50256, 314, 588, 6290, 5173, 592]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, the tokenizer handles the typo"
      ],
      "metadata": {
        "id": "Z2cxUbOOHyto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = oTokenizer.decode(ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NflvrErHvtM",
        "outputId": "e53b7168-7205-4a50-ab59-044b5ac46848"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you? <|endoftext|> I like rainydays\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was possible because of the BPE algorithm without the need of the <|unk|> special token.\n",
        "\n",
        "Now, how about random werid words?"
      ],
      "metadata": {
        "id": "DDFcVbf1IglP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weird_text = \"Aerg Uki Klajfd\"\n",
        "weird_ids = oTokenizer.encode(weird_text)\n",
        "print(weird_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imKRCxFNH3I9",
        "outputId": "8e731c1a-466b-4de5-c885-1d2ee0f1e05b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32, 6422, 471, 4106, 14770, 1228, 16344]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_weird_text = oTokenizer.decode(weird_ids)\n",
        "print(decoded_weird_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCat-eWLI_Th",
        "outputId": "d489b138-5a26-472f-c49c-bf641d1749b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aerg Uki Klajfd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It worked! This is thanks to the BPE algorithm."
      ],
      "metadata": {
        "id": "6eXKFRuTJKUm"
      }
    }
  ]
}